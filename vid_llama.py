import os
import time

import dotenv
import torch
from openai import OpenAI
from openai.types.chat import ChatCompletionMessageParam
from transformers import AutoModelForCausalLM, AutoProcessor

dotenv.load_dotenv()

client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))


def translate_to_vietnamese(text: str) -> str:
    """Analyze English video description and provide Vietnamese fall detection response"""
    messages: list[ChatCompletionMessageParam] = [
        {"role": "system", "content": "B·∫°n l√† tr·ª£ l√Ω AI chuy√™n v·ªÅ ph√°t hi·ªán t√© ng√£ trong m√¥i tr∆∞·ªùng b·ªánh vi·ªán."},
        {
            "role": "user",
            "content": f"""H√£y ph√¢n t√≠ch m√¥ t·∫£ video n√†y v√† x√°c ƒë·ªãnh xem c√≥ x·∫£y ra t√© ng√£ c·ªßa con ng∆∞·ªùi hay kh√¥ng:
{text}

Ch·ªâ tr·∫£ l·ªùi theo m·ªôt trong hai ƒë·ªãnh d·∫°ng sau:
"PH√ÅT_HI·ªÜN_T√â_NG√É: [m√¥ t·∫£ ng·∫Øn g·ªçn v·ªÅ t√© ng√£]"
"KH√îNG_PH√ÅT_HI·ªÜN_T√â_NG√É: [m√¥ t·∫£ ng·∫Øn g·ªçn v·ªÅ ho·∫°t ƒë·ªông b√¨nh th∆∞·ªùng]""",
        },
    ]

    response = client.chat.completions.create(model="gpt-4o-mini", messages=messages, temperature=0)
    return response.choices[0].message.content.strip()


def load_model(model_name: str = "DAMO-NLP-SG/VideoLLaMA3-2B"):
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        trust_remote_code=True,
        device_map="auto",
        torch_dtype=torch.bfloat16,
        attn_implementation="flash_attention_2",
    )
    processor = AutoProcessor.from_pretrained(model_name, trust_remote_code=True)
    return model, processor


def get_video_description(model, processor, video_path: str):
    """Get detailed video description from VideoLLaMA3 in English"""
    question = """Describe this video in detail. Focus on:
    1. What people are doing in the video
    2. Any movements, actions, or activities
    3. Any falls, stumbles, or accidents
    4. Body positions and movements
    5. Environmental context

    Provide a comprehensive description of all activities and movements you observe."""

    conversation = [
        {
            "role": "system",
            "content": "You are an AI system that specializes in detailed video analysis. Analyze the video accurately and provide comprehensive descriptions.",
        },
        {
            "role": "user",
            "content": [
                {"type": "video", "video": {"video_path": video_path, "fps": 1, "max_frames": 64}},
                {"type": "text", "text": question},
            ],
        },
    ]

    inputs = processor(conversation=conversation, return_tensors="pt")
    inputs = {k: v.cuda() if isinstance(v, torch.Tensor) else v for k, v in inputs.items()}
    if "pixel_values" in inputs:
        inputs["pixel_values"] = inputs["pixel_values"].to(torch.bfloat16)

    t2 = time.time()
    output_ids = model.generate(**inputs, max_new_tokens=200)
    t3 = time.time()

    response = processor.batch_decode(output_ids, skip_special_tokens=True)[0].strip()

    # Extract the actual response (remove conversation context)
    if "assistant\n\n" in response:
        response = response.split("assistant\n\n")[-1]
    elif "Assistant:" in response:
        response = response.split("Assistant:")[-1]

    print(f"[VideoLLaMA3 Generation time]: {t3 - t2:.2f} s")

    return response.strip()


if __name__ == "__main__":
    video_path = "media/fall-01-cam0.mp4"
    model_name = "DAMO-NLP-SG/VideoLLaMA3-2B"

    print("üöÄ B·∫Øt ƒë·∫ßu qu√° tr√¨nh ph√¢n t√≠ch 2 b∆∞·ªõc:")
    print("üìπ Step 1: VideoLLaMA3 s·∫Ω m√¥ t·∫£ video chi ti·∫øt")
    print("ü§ñ Step 2: OpenAI s·∫Ω ph√¢n t√≠ch m√¥ t·∫£ ƒë·ªÉ ph√°t hi·ªán t√© ng√£\n")

    # Load VideoLLaMA3 model
    print("‚è≥ ƒêang t·∫£i VideoLLaMA3 model...")
    model, processor = load_model(model_name)
    print("‚úÖ VideoLLaMA3 model ƒë√£ t·∫£i th√†nh c√¥ng!\n")

    # Step 1: Get video description from VideoLLaMA3
    print("üìπ STEP 1: L·∫•y m√¥ t·∫£ chi ti·∫øt t·ª´ VideoLLaMA3...")
    video_description = get_video_description(model, processor, video_path)
    print(f"üìù M√¥ t·∫£ video (VideoLLaMA3): {video_description}\n")

    # Step 2: Analyze with OpenAI for Vietnamese fall detection
    print("ü§ñ STEP 2: Ph√¢n t√≠ch m√¥ t·∫£ b·∫±ng OpenAI ƒë·ªÉ ph√°t hi·ªán t√© ng√£...")
    vietnamese_analysis = translate_to_vietnamese(video_description)
    print(f"üè• K·∫øt qu·∫£ ph√¢n t√≠ch (OpenAI Vietnamese): {vietnamese_analysis}\n")

    print("‚úÖ Ho√†n th√†nh qu√° tr√¨nh ph√¢n t√≠ch 2 b∆∞·ªõc!")
    print("üí° ∆Øu ƒëi·ªÉm: VideoLLaMA3 hi·ªÉu video t·ªët h∆°n, OpenAI ph√¢n t√≠ch ng√¥n ng·ªØ t·ª± nhi√™n t·ªët h∆°n")
