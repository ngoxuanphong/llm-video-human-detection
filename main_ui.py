import asyncio
import json
import os
import queue
import threading
import time
from datetime import datetime

import cv2
import gradio as gr
import numpy as np
from PIL import Image

from src import (
    OPENAI_CLIENT,
    SAVE_ANALYSIS_FRAMES,
    SAVE_FORMAT,
    TELEGRAM_BOT,
    USE_TELE_ALERT,
    alert_services,
)
from src.audio_warning import AudioWarningSystem
from src.utils import frames_to_base64, prepare_messages, save_analysis_frames_to_temp
from src.videollama_detector import VideoLLamaFallDetector
from loguru import logger

class FallDetectionWebUI:
    def __init__(self):
        # Camera and detection settings
        self.camera = None
        self.is_running = False
        self.analysis_interval = 5
        self.frame_buffer = []
        self.last_analysis_time = 0
        self.fall_detected_cooldown = 30
        self.last_fall_alert = 0
        self.frame_count = 0
        self.analysis_count = 0
        self.start_time = time.time()

        # Detection method: "openai" or "videollama3"
        self.detection_method = "openai"

        # Initialize VideoLLaMA3 detector
        self.videollama_detector = VideoLLamaFallDetector()

        # Initialize audio warning system
        self.audio_warning = AudioWarningSystem()

        # UI specific
        self.current_frame = None
        self.alert_history = []
        self.system_logs = []
        self.status_data = {}
        self.ui_update_queue = queue.Queue()

        # Status tracking
        self.camera_status = "Kh√¥ng ho·∫°t ƒë·ªông"
        self.last_analysis_result = "Ch∆∞a c√≥ ph√¢n t√≠ch"

        # Video upload processing
        self.upload_processing = False
        self.uploaded_video_path = None

        # Evidence storage
        self.evidence_gifs = []  # Store paths to saved GIF evidence

    def initialize_camera(self, camera_index=0):
        """Initialize camera capture"""
        try:
            if self.camera:
                self.camera.release()

            self.camera = cv2.VideoCapture(camera_index)
            if not self.camera.isOpened():
                raise Exception(f"Kh√¥ng th·ªÉ m·ªü camera {camera_index}")

            # Set camera properties
            self.camera.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
            self.camera.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
            self.camera.set(cv2.CAP_PROP_FPS, 30)

            self.camera_status = "Ho·∫°t ƒë·ªông"
            self.add_log("‚úì Camera ƒë√£ ƒë∆∞·ª£c kh·ªüi t·∫°o th√†nh c√¥ng", "success")
            return True

        except Exception as e:
            self.camera_status = "L·ªói"
            self.add_log(f"‚úó Kh√¥ng th·ªÉ kh·ªüi t·∫°o camera: {e}", "error")
            return False

    def add_log(self, message, log_type="info"):
        """Add log message with timestamp"""
        timestamp = datetime.now().strftime("%H:%M:%S")
        logger.info(f"[{timestamp}] {log_type} {message}")
        log_entry = {"time": timestamp, "message": message, "type": log_type}
        self.system_logs.append(log_entry)
        # Keep only last 100 logs
        if len(self.system_logs) > 100:
            self.system_logs.pop(0)

    def capture_frames(self):
        """Continuously capture frames from camera"""
        while self.is_running and self.camera:
            ret, frame = self.camera.read()
            if not ret:
                self.add_log("‚ö† Kh√¥ng th·ªÉ ch·ª•p khung h√¨nh", "warning")
                continue

            # Add timestamp to frame
            timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            cv2.putText(frame, timestamp, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)

            # Store frame with timestamp
            current_time = time.time()
            self.frame_count += 1
            self.frame_buffer.append({"frame": frame, "timestamp": current_time})

            # Keep only recent frames (last 10 seconds)
            self.frame_buffer = [f for f in self.frame_buffer if current_time - f["timestamp"] < 10]

            # Update current frame for UI
            self.current_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

            # Check for analysis trigger
            if current_time - self.last_analysis_time >= self.analysis_interval:
                threading.Thread(target=self.analyze_frames, daemon=True).start()
                self.last_analysis_time = current_time

            time.sleep(0.03)  # ~30 FPS

    def analyze_frames(self):
        """Analyze recent frames for fall detection using selected method"""
        if not self.frame_buffer:
            return

        try:
            self.analysis_count += 1
            self.add_log(f"üîç B·∫Øt ƒë·∫ßu ph√¢n t√≠ch l·∫ßn {self.analysis_count}...", "info")

            # Get recent frames
            recent_frames = self.frame_buffer.copy()

            # Save analysis frames if enabled
            if SAVE_ANALYSIS_FRAMES:
                threading.Thread(target=save_analysis_frames_to_temp, args=([recent_frames])).start()

            # Choose analysis method
            if self.detection_method == "videollama3":
                analysis_result = self.analyze_frames_videollama3(recent_frames)
            else:  # Default to OpenAI
                analysis_result = self.analyze_frames_openai(recent_frames)

            if analysis_result:
                self.last_analysis_result = analysis_result
                self.add_log(f"üìä K·∫øt qu·∫£ ph√¢n t√≠ch: {analysis_result}", "info")

                # Check for fall detection (Vietnamese)
                if analysis_result.startswith("PH√ÅT_HI·ªÜN_T√â_NG√É"):
                    self.handle_fall_detection(analysis_result)

        except Exception as e:
            self.add_log(f"‚ùå L·ªói ph√¢n t√≠ch: {e}", "error")

    def analyze_frames_openai(self, recent_frames):
        """Analyze frames using VLM SmolVLM"""
        try:
            base64_frames = frames_to_base64(recent_frames)
            if not base64_frames:
                return None

            # Call OpenAI API
            response = OPENAI_CLIENT.chat.completions.create(model="gpt-4o-mini", messages=prepare_messages(base64_frames), max_tokens=150)

            return response.choices[0].message.content.strip()
        except Exception as e:
            self.add_log(f"‚ùå L·ªói OpenAI API: {e}", "error")
            return None

    def analyze_frames_videollama3(self, recent_frames):
        """Analyze frames using local VideoLLaMA3 model + OpenAI Vietnamese analysis"""
        try:
            if not self.videollama_detector.is_loaded:
                self.add_log("‚ö†Ô∏è VideoLLaMA3 model ch∆∞a ƒë∆∞·ª£c t·∫£i", "warning")
                return None

            # Check if OpenAI is available for Vietnamese analysis
            openai_available = bool(os.environ.get("OPENAI_API_KEY"))
            if not openai_available:
                self.add_log("‚ö†Ô∏è OpenAI API key kh√¥ng c√≥, kh√¥ng th·ªÉ ph√¢n t√≠ch ti·∫øng Vi·ªát", "warning")
                return "L·ªñI_C·∫§U_H√åNH: Thi·∫øu OpenAI API key cho ph√¢n t√≠ch ti·∫øng Vi·ªát"

            self.add_log("üîÑ B·∫Øt ƒë·∫ßu qu√° tr√¨nh ph√¢n t√≠ch 2 b∆∞·ªõc: VideoLLaMA3 ‚Üí OpenAI", "info")

            # Call the combined analysis method
            result = self.videollama_detector.analyze_frames(recent_frames)

            if result.startswith("L·ªñI_PH√ÇN_T√çCH_K·∫æT_H·ª¢P"):
                self.add_log(f"‚ùå L·ªói ph√¢n t√≠ch k·∫øt h·ª£p: {result}", "error")
            elif result.startswith("PH√ÅT_HI·ªÜN_T√â_NG√É"):
                self.add_log("‚úÖ Ho√†n th√†nh ph√¢n t√≠ch 2 b∆∞·ªõc - Ph√°t hi·ªán t√© ng√£!", "success")
            elif result.startswith("KH√îNG_PH√ÅT_HI·ªÜN_T√â_NG√É"):
                self.add_log("‚úÖ Ho√†n th√†nh ph√¢n t√≠ch 2 b∆∞·ªõc - Kh√¥ng c√≥ t√© ng√£", "success")
            else:
                self.add_log("‚ö†Ô∏è K·∫øt qu·∫£ ph√¢n t√≠ch kh√¥ng theo ƒë·ªãnh d·∫°ng mong ƒë·ª£i", "warning")

            return result

        except Exception as e:
            self.add_log(f"‚ùå L·ªói VideoLLaMA3 + OpenAI: {e}", "error")
            return None

    def handle_fall_detection(self, analysis_result):
        """Handle detected fall - send alerts and play audio warning"""
        current_time = time.time()

        # Check cooldown to prevent spam
        if current_time - self.last_fall_alert < self.fall_detected_cooldown:
            self.add_log("‚è≥ Ph√°t hi·ªán t√© ng√£ nh∆∞ng v·∫´n trong th·ªùi gian ch·ªù", "warning")
            return

        self.last_fall_alert = current_time
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

        # Add to alert history
        alert_data = {
            "timestamp": timestamp,
            "details": analysis_result,
            "frame_count": len(self.frame_buffer),
            "evidence_saved": SAVE_ANALYSIS_FRAMES,
            "source": "Live Camera",
            "detection_method": self.detection_method.upper(),
        }
        self.alert_history.append(alert_data)

        # Log the alert
        self.add_log(f"üö® PH√ÅT HI·ªÜN T√â NG√É: {analysis_result}", "alert")

        # Play audio warning (async to avoid blocking)
        self.audio_warning.play_warning_async(analysis_result)
        self.add_log("üîä ƒê√£ ph√°t c·∫£nh b√°o √¢m thanh", "success")

        # Save evidence as GIF
        try:
            gif_folder = self.save_evidence_gif(self.frame_buffer, timestamp, "Live Camera")
            if gif_folder:
                alert_data["gif_evidence"] = gif_folder
                self.evidence_gifs.append(
                    {"path": gif_folder, "timestamp": timestamp, "source": "Live Camera", "details": analysis_result, "detection_method": self.detection_method.upper()}
                )
                self.add_log(f"üíæ ƒê√£ l∆∞u b·∫±ng ch·ª©ng GIF: {os.path.basename(gif_folder)}", "success")
        except Exception as e:
            self.add_log(f"‚ùå L·ªói l∆∞u GIF: {e}", "error")

        # Send Telegram notification only if enabled
        if TELEGRAM_BOT and USE_TELE_ALERT:
            asyncio.create_task(alert_services.send_telegram_alert(analysis_result, timestamp, self.frame_buffer))
            self.add_log("üì± Th√¥ng b√°o Telegram ƒë√£ g·ª≠i", "success")
        else:
            self.add_log("‚Ñπ B·ªè qua th√¥ng b√°o Telegram (ƒë√£ t·∫Øt ho·∫∑c ch∆∞a c·∫•u h√¨nh)", "info")

        # Save current frame as evidence (original format)
        if self.frame_buffer:
            threading.Thread(target=save_analysis_frames_to_temp, args=([self.frame_buffer])).start()

    def start_detection(self, camera_index):
        """Start the fall detection system"""
        if self.is_running:
            return "‚ùå H·ªá th·ªëng ƒë√£ ƒëang ch·∫°y!", self.get_status_info()

        if not self.initialize_camera(camera_index):
            return "‚ùå Kh√¥ng th·ªÉ kh·ªüi t·∫°o camera!", self.get_status_info()

        self.is_running = True
        self.start_time = time.time()
        self.add_log("üöÄ H·ªá th·ªëng ph√°t hi·ªán t√© ng√£ ƒë√£ kh·ªüi ƒë·ªông", "success")

        # Start capture thread
        threading.Thread(target=self.capture_frames, daemon=True).start()

        return "‚úÖ H·ªá th·ªëng ƒë√£ kh·ªüi ƒë·ªông th√†nh c√¥ng!", self.get_status_info()

    def stop_detection(self):
        """Stop the fall detection system"""
        if not self.is_running:
            return "‚ùå H·ªá th·ªëng ch∆∞a ch·∫°y!", self.get_status_info()

        self.is_running = False
        self.camera_status = "ƒê√£ d·ª´ng"

        if self.camera:
            self.camera.release()
            self.camera = None

        self.add_log("üõë H·ªá th·ªëng ph√°t hi·ªán t√© ng√£ ƒë√£ d·ª´ng", "info")
        return "‚úÖ H·ªá th·ªëng ƒë√£ d·ª´ng!", self.get_status_info()

    def get_current_frame(self):
        """Get current frame for display"""
        if self.current_frame is not None:
            return Image.fromarray(self.current_frame)
        else:
            # Return a placeholder image
            placeholder = np.zeros((480, 640, 3), dtype=np.uint8)
            cv2.putText(placeholder, "Camera chua khoi dong", (150, 240), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
            return Image.fromarray(placeholder)

    def get_status_info(self):
        """Get system status information"""
        uptime = time.strftime("%H:%M:%S", time.gmtime(time.time() - self.start_time))

        # Get VideoLLaMA3 status
        llama_status = self.videollama_detector.get_model_status()
        audio_status = self.audio_warning.get_status()

        # Check OpenAI availability for VideoLLaMA3 method
        openai_available = bool(os.environ.get("OPENAI_API_KEY"))

        status_text = f"""
üìä **TR·∫†NG TH√ÅI H·ªÜ TH·ªêNG**

üé• **Camera:** {self.camera_status}

ü§ñ **Ph∆∞∆°ng th·ª©c ph√°t hi·ªán:** {"SmolVLM" if self.detection_method == "openai" else "VideoLLaMA3 + OpenAI"}

üîç **L·∫ßn ph√¢n t√≠ch:** {self.analysis_count}

üì± **G·ª≠i Tin Nh·∫Øn:** {'B·∫≠t' if USE_TELE_ALERT else 'T·∫Øt'}

üíæ **L∆∞u Frames:** {'B·∫≠t (' + SAVE_FORMAT.upper() + ')' if SAVE_ANALYSIS_FRAMES else 'T·∫Øt'}

‚è∞ **Th·ªùi gian ho·∫°t ƒë·ªông:** {uptime}

üîÑ **Chu k·ª≥:** {self.analysis_interval}s

üìà **Khung h√¨nh/Buffer Frames:** {self.frame_count} / {len(self.frame_buffer)}

üö® **C·∫£nh b√°o:** {len(self.alert_history)}

üîä **Audio Warning:** {'‚úÖ Enabled' if audio_status['enabled'] else '‚ùå Disabled'} ({audio_status['tts_method']})

üìã **K·∫øt qu·∫£ ph√¢n t√≠ch g·∫ßn nh·∫•t:**
{self.last_analysis_result}
        """
        return status_text

    def get_logs_display(self):
        """Get formatted logs for display"""
        if not self.system_logs:
            return "Ch∆∞a c√≥ log n√†o..."

        log_text = ""
        for log in self.system_logs[-30:]:  # Show last 30 logs
            emoji = {"info": "‚ÑπÔ∏è", "success": "‚úÖ", "warning": "‚ö†Ô∏è", "error": "‚ùå", "alert": "üö®"}
            icon = emoji.get(log["type"], "üìù")
            log_text += f"[{log['time']}] {icon} {log['message']}\n"

        return log_text

    def get_alert_history_display(self):
        """Get formatted alert history for display"""
        if not self.alert_history:
            return "Ch∆∞a c√≥ c·∫£nh b√°o n√†o..."

        alert_text = ""
        for i, alert in enumerate(reversed(self.alert_history[-10:])):  # Show last 10 alerts
            alert_text += f"""
**C·∫£nh b√°o #{len(self.alert_history) - i}**
üïê Th·ªùi gian: {alert['timestamp']}
üìù Chi ti·∫øt: {alert['details']}
üìä Frames: {alert['frame_count']}
üíæ B·∫±ng ch·ª©ng: {'ƒê√£ l∆∞u' if alert['evidence_saved'] else 'Kh√¥ng l∆∞u'}
---
            """

        return alert_text

    def process_uploaded_video(self, video_path):
        """Process uploaded video file for fall detection - analyze entire video as one piece"""
        if not video_path:
            return "‚ùå Kh√¥ng c√≥ video ƒë∆∞·ª£c upload!", "Vui l√≤ng ch·ªçn file video"

        if self.upload_processing:
            return "‚ùå ƒêang x·ª≠ l√Ω video kh√°c!", "Vui l√≤ng ch·ªù ho√†n th√†nh"

        self.upload_processing = True
        self.uploaded_video_path = video_path

        try:
            self.add_log(f"üìÅ B·∫Øt ƒë·∫ßu ph√¢n t√≠ch to√†n b·ªô video: {os.path.basename(video_path)}", "info")

            # Open video file
            cap = cv2.VideoCapture(video_path)
            if not cap.isOpened():
                raise Exception("Kh√¥ng th·ªÉ m·ªü file video")

            # Get video properties
            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            fps = cap.get(cv2.CAP_PROP_FPS)
            duration = total_frames / fps if fps > 0 else 0

            self.add_log(f"üìä Video info: {total_frames} frames, {fps:.1f} FPS, {duration:.1f}s", "info")

            # Read all frames for complete analysis
            frame_buffer = []
            frame_count = 0

            # Sample frames to avoid memory issues (max 60 frames for analysis)
            sample_interval = max(1, total_frames // 60) if total_frames > 60 else 1

            while True:
                ret, frame = cap.read()
                if not ret:
                    break

                # frame_count += 1
                # current_time = frame_count / fps if fps > 0 else frame_count * 0.033

                # # Sample frames at intervals to keep memory usage reasonable
                # if frame_count % sample_interval == 0:
                current_time = frame_count / fps if fps > 0 else frame_count * 0.033
                frame_buffer.append({"frame": frame, "timestamp": current_time})

                # Update progress

            cap.release()

            if not frame_buffer:
                raise Exception("Kh√¥ng th·ªÉ ƒë·ªçc frame n√†o t·ª´ video")

            self.add_log(f"üìä ƒê√£ sample {len(frame_buffer)} frames t·ª´ {frame_count} frames t·ªïng", "info")

            # Analyze the entire video as one piece
            self.add_log("üîç B·∫Øt ƒë·∫ßu ph√¢n t√≠ch to√†n b·ªô video...", "info")

            analysis_result = self.analyze_video_frames(frame_buffer, 1, video_path)

            # Display result prominently
            result_summary = ""
            if analysis_result:
                self.last_analysis_result = analysis_result
                self.add_log(f"üìä K·∫øt qu·∫£ ph√¢n t√≠ch: {analysis_result}", "info")

                # Check for fall detection
                if "PH√ÅT_HI·ªÜN_T√â_NG√É" in analysis_result:
                    self.handle_video_fall_detection(analysis_result, frame_buffer, duration / 2, video_path)
                    result_summary = f"üö® T√â NG√É ƒê∆Ø·ª¢C PH√ÅT HI·ªÜN!\n{analysis_result}"
                elif "KH√îNG_PH√ÅT_HI·ªÜN_T√â_NG√É" in analysis_result:
                    result_summary = f"‚úÖ KH√îNG C√ì T√â NG√É\n{analysis_result}"
                else:
                    result_summary = f"üìä K·∫æT QU·∫¢ PH√ÇN T√çCH\n{analysis_result}"
            else:
                result_summary = "‚ùå Kh√¥ng th·ªÉ ph√¢n t√≠ch video"

            self.upload_processing = False

            completion_msg = f"‚úÖ Ho√†n th√†nh ph√¢n t√≠ch video!\nFrames g·ªëc: {frame_count}\nFrames ph√¢n t√≠ch: {len(frame_buffer)}"
            self.add_log(completion_msg, "success")

            video_info = f"""üìπ Video: {os.path.basename(video_path)}
‚è±Ô∏è Th·ªùi l∆∞·ª£ng: {duration:.1f}s
üìä Frames: {frame_count} (ph√¢n t√≠ch {len(frame_buffer)})
ü§ñ Ph∆∞∆°ng th·ª©c: {self.detection_method.upper()}

{result_summary}"""

            return completion_msg, video_info

        except Exception as e:
            self.upload_processing = False
            error_msg = f"‚ùå L·ªói x·ª≠ l√Ω video: {e}"
            self.add_log(error_msg, "error")
            return error_msg, f"X·ª≠ l√Ω th·∫•t b·∫°i: {str(e)}"

    def analyze_video_frames(self, frame_buffer, analysis_count, source_video):
        """Analyze frames from uploaded video"""
        if not frame_buffer:
            return None

        try:
            self.add_log(f"üîç Ph√¢n t√≠ch video ho√†n ch·ªânh ({len(frame_buffer)} frames) - {self.detection_method.upper()}...", "info")

            # Choose analysis method
            if self.detection_method == "videollama3":
                if not self.videollama_detector.is_loaded:
                    self.add_log("‚ö†Ô∏è VideoLLaMA3 model ch∆∞a ƒë∆∞·ª£c t·∫£i, chuy·ªÉn v·ªÅ OpenAI", "warning")
                    return self.analyze_video_frames_openai(frame_buffer)
                else:
                    return self.videollama_detector.analyze_frames(frame_buffer)
            else:
                return self.analyze_video_frames_openai(frame_buffer)

        except Exception as e:
            self.add_log(f"‚ùå L·ªói ph√¢n t√≠ch video frames: {e}", "error")
            return None

    def analyze_video_frames_openai(self, frame_buffer):
        """Analyze video frames using OpenAI"""
        # Sample frames to avoid too many (max 8 frames for better analysis)
        if len(frame_buffer) > 16:
            step = len(frame_buffer) // 16
            sample_frames = frame_buffer[::step][:16]  # Take exactly 8 frames
        else:
            sample_frames = frame_buffer

        base64_frames = frames_to_base64(sample_frames)

        if not base64_frames:
            return None

        self.add_log(f"üì§ G·ª≠i {len(base64_frames)} frames t·ªõi OpenAI ƒë·ªÉ ph√¢n t√≠ch...", "info")

        # Call OpenAI API
        response = OPENAI_CLIENT.chat.completions.create(model="gpt-4o-mini", messages=prepare_messages(base64_frames), max_tokens=150)

        analysis_result = response.choices[0].message.content.strip()
        self.add_log(f"üìä K·∫øt qu·∫£ ph√¢n t√≠ch OpenAI: {analysis_result}", "info")
        return analysis_result

    def handle_video_fall_detection(self, analysis_result, frame_buffer, timestamp, source_video):
        """Handle fall detection from uploaded video"""
        detection_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

        # Create alert data for video
        alert_data = {
            "timestamp": detection_time,
            "details": analysis_result,
            "frame_count": len(frame_buffer),
            "evidence_saved": True,
            "source": f"Video: {os.path.basename(source_video)}",
            "video_timestamp": f"{timestamp:.1f}s",
            "detection_method": self.detection_method.upper(),
        }

        self.alert_history.append(alert_data)
        self.add_log(f"üö® PH√ÅT HI·ªÜN T√â NG√É TRONG VIDEO: {analysis_result} t·∫°i {timestamp:.1f}s", "alert")

        # Play audio warning for video detection too
        self.audio_warning.play_warning_async(analysis_result)
        self.add_log("üîä ƒê√£ ph√°t c·∫£nh b√°o √¢m thanh cho video", "success")

        # Save evidence as GIF
        try:
            gif_folder = self.save_evidence_gif(frame_buffer, detection_time, source_video)
            if gif_folder:
                alert_data["gif_evidence"] = gif_folder
                self.evidence_gifs.append(
                    {
                        "path": gif_folder,
                        "timestamp": detection_time,
                        "source": os.path.basename(source_video),
                        "details": analysis_result,
                        "detection_method": self.detection_method.upper(),
                    }
                )
                self.add_log(f"üíæ ƒê√£ l∆∞u b·∫±ng ch·ª©ng GIF: {os.path.basename(gif_folder)}", "success")
        except Exception as e:
            self.add_log(f"‚ùå L·ªói l∆∞u GIF: {e}", "error")

        # Send Telegram notification if enabled
        if TELEGRAM_BOT and USE_TELE_ALERT:
            try:
                asyncio.create_task(
                    alert_services.send_telegram_alert(
                        f"üé¨ {analysis_result} (Video: {os.path.basename(source_video)} t·∫°i {timestamp:.1f}s)", detection_time, frame_buffer
                    )
                )
                self.add_log("üì± Th√¥ng b√°o Telegram ƒë√£ g·ª≠i", "success")
            except Exception as e:
                self.add_log(f"‚ùå L·ªói g·ª≠i Telegram: {e}", "error")

    def save_evidence_gif(self, frame_buffer, timestamp, source_info):
        """Save frame buffer as GIF evidence with same format as temp folder"""
        try:
            # Create evidence directory if not exists
            evidence_dir = "evidence_gifs"
            os.makedirs(evidence_dir, exist_ok=True)

            # Create timestamp folder (matching temp folder format)
            safe_timestamp = timestamp.replace(":", "").replace(" ", "_").replace("-", "")
            timestamp_folder = os.path.join(evidence_dir, f"fall_{safe_timestamp}")
            os.makedirs(timestamp_folder, exist_ok=True)

            # Generate GIF filename
            gif_filename = "fall_evidence.gif"
            gif_path = os.path.join(timestamp_folder, gif_filename)

            # Generate info filename
            info_filename = "fall_info.txt"
            info_path = os.path.join(timestamp_folder, info_filename)

            # Convert frames to PIL Images with proper timing
            pil_frames = []
            frame_timestamps = []

            for i, frame_data in enumerate(frame_buffer):
                frame = frame_data["frame"]
                # Resize frame for consistent size (matching temp folder)
                height, width = frame.shape[:2]
                new_width = min(640, width)  # Max width 640px
                new_height = int(height * (new_width / width))
                resized_frame = cv2.resize(frame, (new_width, new_height))

                # Convert BGR to RGB
                pil_frame = Image.fromarray(cv2.cvtColor(resized_frame, cv2.COLOR_BGR2RGB))
                pil_frames.append(pil_frame)
                frame_timestamps.append(frame_data.get("timestamp", i * 0.1))

            # Save as GIF with proper timing (50 FPS for 5x speed)
            if pil_frames:
                duration_ms = 20  # 20ms per frame = 50 FPS (5x faster than before)
                pil_frames[0].save(gif_path, save_all=True, append_images=pil_frames[1:], duration=duration_ms, loop=0)

                # Create info file (matching temp folder format)
                gif_duration = len(pil_frames) * 0.02  # 50 FPS
                info_content = f"""Fall Detection Evidence
========================================
Timestamp: {timestamp}
Source: {source_info}
Detection Method: {self.detection_method.upper()}
Total frames: {len(pil_frames)}
Save format: gif
GIF duration: {gif_duration:.1f}s (50 FPS - 5x speed)

Files saved:
- {gif_filename}

Analysis Result:
{self.last_analysis_result}
"""

                with open(info_path, "w", encoding="utf-8") as f:
                    f.write(info_content)

                # Return the folder path (not just GIF path) for consistency
                return timestamp_folder

        except Exception as e:
            self.add_log(f"‚ùå L·ªói t·∫°o GIF evidence: {e}", "error")
            return None

    def get_evidence_gifs_display(self):
        """Get list of evidence GIFs for display"""
        if not self.evidence_gifs:
            return "Ch∆∞a c√≥ b·∫±ng ch·ª©ng GIF n√†o...", [], []

        # Create list of evidence items for selection
        evidence_choices = []
        gif_paths = []

        for i, evidence in enumerate(reversed(self.evidence_gifs)):  # Most recent first
            evidence_index = len(self.evidence_gifs) - i
            choice_text = f"#{evidence_index}: {evidence['timestamp']} - {evidence['source']}"
            evidence_choices.append(choice_text)  # Just the text, not tuple

            # Look for GIF file in the evidence folder
            evidence_folder = evidence["path"]
            if os.path.isdir(evidence_folder):
                gif_file = os.path.join(evidence_folder, "fall_evidence.gif")
                if os.path.exists(gif_file):
                    gif_paths.append(gif_file)
            elif evidence_folder.endswith(".gif") and os.path.exists(evidence_folder):
                gif_paths.append(evidence_folder)

        summary_text = f"üìÅ **C√ì {len(self.evidence_gifs)} B·∫∞NG CH·ª®NG GIF**\n\nCh·ªçn m·ªôt m·ª•c t·ª´ danh s√°ch b√™n d∆∞·ªõi ƒë·ªÉ xem chi ti·∫øt."

        return summary_text, evidence_choices, gif_paths

    def get_evidence_details(self, selected_choice):
        """Get detailed information for selected evidence"""
        if not selected_choice or not self.evidence_gifs:
            return "Kh√¥ng c√≥ th√¥ng tin", None

        try:
            # Extract the evidence number from the choice text (e.g., "#1: ..." -> 1)
            if selected_choice.startswith("#"):
                evidence_number = int(selected_choice.split(":")[0][1:])
                # Convert to index (evidence_number is 1-based, we need 0-based index from reversed list)
                selected_index = len(self.evidence_gifs) - evidence_number

                if selected_index < 0 or selected_index >= len(self.evidence_gifs):
                    return "Ch·ªâ s·ªë kh√¥ng h·ª£p l·ªá", None

                evidence = self.evidence_gifs[selected_index]
            else:
                return "ƒê·ªãnh d·∫°ng kh√¥ng h·ª£p l·ªá", None

        except (ValueError, IndexError):
            return "L·ªói ph√¢n t√≠ch l·ª±a ch·ªçn", None

        details_text = f"""
**üé¨ B·∫∞NG CH·ª®NG #{evidence_number}**

üìÖ **Th·ªùi gian:** {evidence['timestamp']}
üìÅ **Ngu·ªìn:** {evidence['source']}
ü§ñ **Ph∆∞∆°ng th·ª©c:** {evidence.get('detection_method', 'UNKNOWN')}
üìù **Chi ti·∫øt:** {evidence['details']}
üìÑ **Th∆∞ m·ª•c:** {os.path.basename(evidence['path'])}

---
‚ú® **GIF ƒë∆∞·ª£c tƒÉng t·ªëc 5x ƒë·ªÉ xem nhanh h∆°n**
üéØ **Ph√¢n t√≠ch:** {evidence['details']}
        """

        # Get GIF path
        evidence_folder = evidence["path"]
        gif_path = None
        if os.path.isdir(evidence_folder):
            gif_file = os.path.join(evidence_folder, "fall_evidence.gif")
            if os.path.exists(gif_file):
                gif_path = gif_file
        elif evidence_folder.endswith(".gif") and os.path.exists(evidence_folder):
            gif_path = evidence_folder

        return details_text, gif_path

    def get_upload_progress(self):
        """Get current upload processing progress"""
        if not self.upload_processing:
            return "S·∫µn s√†ng upload video", 0
        return f"ƒêang x·ª≠ l√Ω video... {self.upload_progress}%", self.upload_progress

    def set_detection_method(self, method):
        """Set detection method (openai or videollama3)"""
        if method in ["openai", "videollama3"]:
            self.detection_method = method
            self.add_log(f"üîß ƒê√£ chuy·ªÉn ph∆∞∆°ng th·ª©c ph√°t hi·ªán: {method.upper()}", "info")
            return f"‚úÖ ƒê√£ chuy·ªÉn sang {method.upper()}"
        else:
            return "‚ùå Ph∆∞∆°ng th·ª©c kh√¥ng h·ª£p l·ªá"

    def load_videollama3_model(self):
        """Load VideoLLaMA3 model"""
        if self.videollama_detector.is_loaded:
            return "‚ö†Ô∏è Model ƒë√£ ƒë∆∞·ª£c t·∫£i r·ªìi"

        self.add_log("üöÄ ƒêang t·∫£i VideoLLaMA3 model...", "info")

        def load_worker():
            success = self.videollama_detector.load_model()
            if success:
                self.add_log("‚úÖ VideoLLaMA3 model ƒë√£ t·∫£i th√†nh c√¥ng", "success")
                # Force UI update by updating the queue
                self.ui_update_queue.put(("model_loaded", "‚úÖ VideoLLaMA3 model ƒë√£ t·∫£i th√†nh c√¥ng"))
            else:
                self.add_log("‚ùå Kh√¥ng th·ªÉ t·∫£i VideoLLaMA3 model", "error")
                self.ui_update_queue.put(("model_error", "‚ùå Kh√¥ng th·ªÉ t·∫£i VideoLLaMA3 model"))

        threading.Thread(target=load_worker, daemon=True).start()
        return "‚è≥ ƒêang t·∫£i model, vui l√≤ng ch·ªù..."

    def unload_videollama3_model(self):
        """Unload VideoLLaMA3 model"""
        if not self.videollama_detector.is_loaded:
            return "‚ö†Ô∏è Model ch∆∞a ƒë∆∞·ª£c t·∫£i"

        self.videollama_detector.unload_model()
        self.add_log("üóëÔ∏è ƒê√£ g·ª° VideoLLaMA3 model", "info")
        self.ui_update_queue.put(("model_unloaded", "‚úÖ ƒê√£ g·ª° model kh·ªèi b·ªô nh·ªõ"))
        return "‚úÖ ƒê√£ g·ª° model kh·ªèi b·ªô nh·ªõ"

    def get_model_status_message(self):
        """Get current model status for UI display"""
        if self.videollama_detector.is_loaded:
            return "‚úÖ VideoLLaMA3 model ƒë√£ s·∫µn s√†ng"
        else:
            return "‚ùå VideoLLaMA3 model ch∆∞a ƒë∆∞·ª£c t·∫£i"

    def test_audio_warning(self):
        """Test audio warning system"""
        result = self.audio_warning.test_audio_system()
        self.add_log(f"üîä Test audio: {result}", "info")
        return result

    def set_audio_enabled(self, enabled):
        """Enable/disable audio warnings"""
        self.audio_warning.set_enabled(enabled)
        status = "b·∫≠t" if enabled else "t·∫Øt"
        self.add_log(f"üîä C·∫£nh b√°o √¢m thanh ƒë√£ {status}", "info")
        return f"‚úÖ C·∫£nh b√°o √¢m thanh ƒë√£ {status}"

    def set_audio_volume(self, volume):
        """Set audio volume"""
        self.audio_warning.set_volume(volume / 100.0)  # Convert from percentage
        self.add_log(f"üîä √Çm l∆∞·ª£ng ƒë√£ ƒë·∫∑t: {volume}%", "info")
        return f"‚úÖ √Çm l∆∞·ª£ng: {volume}%"


# Initialize the system
fall_system = FallDetectionWebUI()


def create_interface():
    """Create the Gradio interface"""

    with gr.Blocks(
        title="üè• H·ªá Th·ªëng Ph√°t Hi·ªán T√© Ng√£",
        theme=gr.themes.Soft(),
        css="""
        .alert-box { background-color: #ffebee; border-left: 4px solid #f44336; padding: 10px; }
        .success-box { background-color: #e8f5e8; border-left: 4px solid #4caf50; padding: 10px; }
        .info-box { background-color: #e3f2fd; border-left: 4px solid #2196f3; padding: 10px; }
        """,
    ) as demo:

        gr.HTML(
            """
        <div style="text-align: center; margin-bottom: 20px;">
            <h1>üè• H·ªÜ TH·ªêNG PH√ÅT HI·ªÜN H√ÄNH VI NG√É C·ª¶A CON NG∆Ø·ªúI B·∫∞NG VLM</h1>
            <p style="font-size: 16px; color: #666;">
                H·ªá th·ªëng AI ph√°t hi·ªán t√© ng√£ th·ªùi gian th·ª±c s·ª≠ d·ª•ng Vision Language Model
            </p>
        </div>
        """
        )

        with gr.Tab("üé• Camera & ƒêi·ªÅu Khi·ªÉn"):
            with gr.Row():
                with gr.Column(scale=2):
                    camera_feed = gr.Image(label="üìπ Camera Feed", type="pil", height=400, show_label=True)

                    with gr.Row():
                        camera_index = gr.Number(label="Ch·ªâ s·ªë Camera", value=0, precision=0, minimum=0, maximum=10)
                        start_btn = gr.Button("üöÄ Kh·ªüi ƒê·ªông", variant="primary", size="lg")
                        stop_btn = gr.Button("üõë D·ª´ng", variant="secondary", size="lg")

                with gr.Column(scale=1):
                    status_display = gr.Markdown(label="üìä Tr·∫°ng Th√°i H·ªá Th·ªëng", value=fall_system.get_status_info())

                    control_output = gr.Textbox(label="üì¢ Th√¥ng B√°o H·ªá Th·ªëng", value="S·∫µn s√†ng kh·ªüi ƒë·ªông...", interactive=False)

        with gr.Tab("üìã Nh·∫≠t K√Ω H·ªá Th·ªëng"):
            logs_display = gr.Textbox(label="üìù System Logs (30 m·ª•c g·∫ßn nh·∫•t)", value=fall_system.get_logs_display(), lines=30, interactive=False, max_lines=25)

            refresh_logs_btn = gr.Button("üîÑ C·∫≠p Nh·∫≠t Logs", size="sm")

        with gr.Tab("üö® L·ªãch S·ª≠ C·∫£nh B√°o"):
            alert_display = gr.Markdown(label="üö® L·ªãch S·ª≠ T√© Ng√£ (10 c·∫£nh b√°o g·∫ßn nh·∫•t)", value=fall_system.get_alert_history_display())

            refresh_alerts_btn = gr.Button("üîÑ C·∫≠p Nh·∫≠t C·∫£nh B√°o", size="sm")

            with gr.Row():
                clear_alerts_btn = gr.Button("üóëÔ∏è X√≥a L·ªãch S·ª≠", variant="secondary")
                export_alerts_btn = gr.Button("üìÅ Xu·∫•t B√°o C√°o", variant="primary")

        with gr.Tab("üìÅ Upload Video"):
            gr.HTML(
                """
            <div class="info-box">
                <h3>üìÅ Ph√¢n T√≠ch Video T·ª´ File</h3>
                <p>Upload video t·ª´ thi·∫øt b·ªã ƒë·ªÉ ph√¢n t√≠ch t√© ng√£. H·ªó tr·ª£ c√°c ƒë·ªãnh d·∫°ng: MP4, AVI, MOV, MKV</p>
            </div>
            """
            )

            with gr.Row():
                with gr.Column(scale=2):
                    video_upload = gr.File(label="üìπ Ch·ªçn File Video", file_types=["video"], type="filepath")

                    upload_btn = gr.Button("üîç Ph√¢n T√≠ch Video", variant="primary", size="lg")

                    gr.Progress()
                    upload_status = gr.Textbox(label="üìä Tr·∫°ng Th√°i X·ª≠ L√Ω", value="Ch∆∞a c√≥ video ƒë∆∞·ª£c upload...", interactive=False)

                with gr.Column(scale=1):
                    upload_info = gr.Textbox(label="‚ÑπÔ∏è Th√¥ng Tin Video", value="Ch∆∞a ch·ªçn video...", interactive=False, lines=10)

            gr.HTML(
                """
            <div class="alert-box">
                <h4>‚ö†Ô∏è L∆∞u √ù Quan Tr·ªçng</h4>
                <ul>
                    <li>Video s·∫Ω ƒë∆∞·ª£c ph√¢n t√≠ch to√†n b·ªô nh∆∞ m·ªôt ƒëo·∫°n duy nh·∫•t</li>
                    <li>H·ªá th·ªëng s·∫Ω sample t·ªëi ƒëa 60 frames ƒë·ªÉ ph√¢n t√≠ch (tr√°nh qu√° t·∫£i b·ªô nh·ªõ)</li>
                    <li>K·∫øt qu·∫£ s·∫Ω hi·ªÉn th·ªã ngay trong ph·∫ßn th√¥ng tin video</li>
                    <li>GIF b·∫±ng ch·ª©ng s·∫Ω ƒë∆∞·ª£c t·ª± ƒë·ªông t·∫°o khi ph√°t hi·ªán t√© ng√£</li>
                    <li>Th·ªùi gian x·ª≠ l√Ω ph·ª• thu·ªôc v√†o ƒë·ªô d√†i video v√† ph∆∞∆°ng th·ª©c ph√°t hi·ªán</li>
                </ul>
            </div>
            """
            )

        with gr.Tab("üé¨ B·∫±ng Ch·ª©ng GIF"):
            gr.HTML(
                """
            <div class="success-box">
                <h3>üé¨ B·∫±ng Ch·ª©ng T√© Ng√£ (GIF)</h3>
                <p>Xem c√°c GIF b·∫±ng ch·ª©ng ƒë√£ ƒë∆∞·ª£c l∆∞u khi ph√°t hi·ªán t√© ng√£ - TƒÉng t·ªëc 5x</p>
            </div>
            """
            )

            with gr.Row():
                with gr.Column(scale=1):
                    evidence_summary = gr.Markdown(label="üìã T·ªïng Quan", value=fall_system.get_evidence_gifs_display()[0])

                    evidence_selector = gr.Dropdown(
                        label="üéØ Ch·ªçn B·∫±ng Ch·ª©ng", choices=fall_system.get_evidence_gifs_display()[1], value=None, info="Ch·ªçn m·ªôt b·∫±ng ch·ª©ng ƒë·ªÉ xem chi ti·∫øt"
                    )

                    with gr.Row():
                        refresh_evidence_btn = gr.Button("üîÑ C·∫≠p Nh·∫≠t", size="sm")
                        clear_evidence_btn = gr.Button("üóëÔ∏è X√≥a T·∫•t C·∫£", variant="secondary", size="sm")

                with gr.Column(scale=2):
                    evidence_details = gr.Markdown(label="üìù Chi Ti·∫øt B·∫±ng Ch·ª©ng", value="Ch·ªçn m·ªôt b·∫±ng ch·ª©ng t·ª´ danh s√°ch ƒë·ªÉ xem chi ti·∫øt...")

                    evidence_gif_display = gr.Image(label="üé¨ GIF B·∫±ng Ch·ª©ng (5x Speed)", type="filepath", value=None, height=400)

            gr.HTML(
                """
            <div class="info-box">
                <h4>üìã H∆∞·ªõng D·∫´n S·ª≠ D·ª•ng</h4>
                <ul>
                    <li>üéØ Ch·ªçn b·∫±ng ch·ª©ng t·ª´ dropdown ƒë·ªÉ xem chi ti·∫øt</li>
                    <li>üé¨ GIF ƒë∆∞·ª£c tƒÉng t·ªëc 5x ƒë·ªÉ xem nhanh h∆°n</li>
                    <li>üì± GIF t·ª± ƒë·ªông t·∫°o khi ph√°t hi·ªán t√© ng√£</li>
                    <li>üíæ GIF l∆∞u trong th∆∞ m·ª•c <code>evidence_gifs/</code></li>
                    <li>üîÑ Nh·∫•n "C·∫≠p Nh·∫≠t" ƒë·ªÉ l√†m m·ªõi danh s√°ch</li>
                </ul>
            </div>
            """
            )

        with gr.Tab("ü§ñ C·∫•u H√¨nh AI & √Çm Thanh"):
            gr.HTML(
                """
            <div class="info-box">
                <h3>ü§ñ Qu·∫£n L√Ω M√¥ H√¨nh AI</h3>
                <p>Ch·ªçn ph∆∞∆°ng th·ª©c ph√°t hi·ªán t√© ng√£ v√† qu·∫£n l√Ω m√¥ h√¨nh VideoLLaMA3 local</p>
            </div>
            """
            )

            with gr.Row():
                with gr.Column(scale=1):
                    gr.Markdown("### üß† Ph∆∞∆°ng Th·ª©c Ph√°t Hi·ªán")

                    detection_method = gr.Radio(
                        choices=[("VLM SmolVLM", "openai"), ("VideoLLaMA3 + OpenAI", "videollama3")],
                        value="openai",
                        label="Ch·ªçn ph∆∞∆°ng th·ª©c ph√°t hi·ªán",
                        info="SmolVLM: Tr·ª±c ti·∫øp ph√¢n t√≠ch. VideoLLaMA3: M√¥ t·∫£ video ‚Üí OpenAI ph√¢n t√≠ch ti·∫øng Vi·ªát",
                    )

                    method_output = gr.Textbox(label="üì¢ Tr·∫°ng Th√°i Ph∆∞∆°ng Th·ª©c", interactive=False)

                    gr.Markdown("### üéØ VideoLLaMA3 Model")

                    with gr.Row():
                        load_model_btn = gr.Button("üì• T·∫£i Model", variant="primary")
                        unload_model_btn = gr.Button("üóëÔ∏è G·ª° Model", variant="secondary")

                    model_output = gr.Textbox(label="üì¢ Tr·∫°ng Th√°i Model", interactive=False)

                with gr.Column(scale=1):
                    gr.Markdown("### üîä C·∫£nh B√°o √Çm Thanh")

                    audio_enabled = gr.Checkbox(label="üîä B·∫≠t c·∫£nh b√°o √¢m thanh", value=True, info="T·ª± ƒë·ªông ph√°t c·∫£nh b√°o khi ph√°t hi·ªán t√© ng√£")

                    audio_volume = gr.Slider(minimum=0, maximum=100, value=80, step=5, label="üîâ √Çm l∆∞·ª£ng (%)", info="ƒêi·ªÅu ch·ªânh √¢m l∆∞·ª£ng c·∫£nh b√°o")

                    with gr.Row():
                        test_audio_btn = gr.Button("üîä Test √Çm Thanh", variant="primary")
                        audio_output = gr.Textbox(label="üì¢ Tr·∫°ng Th√°i Audio", interactive=False)

            gr.HTML(
                """
            <div class="alert-box">
                <h4>‚ö†Ô∏è L∆∞u √ù Quan Tr·ªçng</h4>
                <ul>
                    <li><strong>OpenAI:</strong> C·∫ßn API key v√† k·∫øt n·ªëi internet, t·ªëc ƒë·ªô ph√¢n t√≠ch nhanh</li>
                    <li><strong>VideoLLaMA3:</strong> Ph√¢n t√≠ch 2 b∆∞·ªõc - VideoLLaMA3 m√¥ t·∫£ video (offline) ‚Üí OpenAI ph√¢n t√≠ch ti·∫øng Vi·ªát (online)</li>
                    <li><strong>Y√™u c·∫ßu VideoLLaMA3:</strong> C·∫ßn c·∫£ VideoLLaMA3 model V√Ä OpenAI API key ƒë·ªÉ ho·∫°t ƒë·ªông</li>
                    <li><strong>Audio:</strong> C·∫ßn c√†i ƒë·∫∑t espeak-ng: <code>sudo apt-get install espeak-ng</code></li>
                    <li><strong>RAM:</strong> VideoLLaMA3 c·∫ßn ~4-8GB VRAM ƒë·ªÉ ch·∫°y m∆∞·ª£t</li>
                </ul>
            </div>
            """
            )

        with gr.Tab("‚öôÔ∏è C·∫•u H√¨nh"):
            gr.HTML(
                """
            <div class="info-box">
                <h3>üîß C·∫•u H√¨nh H·ªá Th·ªëng</h3>
                <p>C√°c c·∫•u h√¨nh ƒë∆∞·ª£c ƒë·ªçc t·ª´ file <code>.env</code>. Sau khi thay ƒë·ªïi, vui l√≤ng kh·ªüi ƒë·ªông l·∫°i ·ª©ng d·ª•ng.</p>
            </div>
            """
            )

            with gr.Row():
                with gr.Column():
                    gr.Markdown(
                        """

                    ### üì± C·∫•u H√¨nh Telegram
                    - **USE_TELE_ALERT**: true/false (m·∫∑c ƒë·ªãnh: false)
                    - **TELEGRAM_BOT_TOKEN**: Token bot Telegram
                    - **TELEGRAM_CHAT_ID**: ID chat nh·∫≠n th√¥ng b√°o

                    ### üíæ C·∫•u H√¨nh L∆∞u Tr·ªØ
                    - **SAVE_ANALYSIS_FRAMES**: true/false (m·∫∑c ƒë·ªãnh: false)
                    - **SAVE_FORMAT**: images/gif/video/all (m·∫∑c ƒë·ªãnh: images)
                    - **MAX_FRAMES**: S·ªë frame t·ªëi ƒëa g·ª≠i AI (m·∫∑c ƒë·ªãnh: 5)
                    """
                    )

                with gr.Column():
                    current_config = f"""
### üìä C·∫•u H√¨nh Hi·ªán T·∫°i
- **Telegram**: {'‚úÖ B·∫≠t' if USE_TELE_ALERT else '‚ùå T·∫Øt'}
- **L∆∞u Frames**: {'‚úÖ B·∫≠t' if SAVE_ANALYSIS_FRAMES else '‚ùå T·∫Øt'}
- **ƒê·ªãnh D·∫°ng**: {SAVE_FORMAT.upper()}
- **Max Frames**: {os.environ.get('MAX_FRAMES', 5)}
                    """
                    gr.Markdown(current_config)

        # Event handlers
        def start_system(camera_idx):
            message, status = fall_system.start_detection(int(camera_idx))
            return message, status

        def stop_system():
            message, status = fall_system.stop_detection()
            return message, status

        def clear_alert_history():
            fall_system.alert_history.clear()
            fall_system.add_log("üóëÔ∏è ƒê√£ x√≥a l·ªãch s·ª≠ c·∫£nh b√°o", "info")
            return "‚úÖ ƒê√£ x√≥a l·ªãch s·ª≠ c·∫£nh b√°o!", fall_system.get_alert_history_display()

        def export_alert_report():
            if not fall_system.alert_history:
                return "‚ùå Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ xu·∫•t!"

            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"alert_report_{timestamp}.json"

            try:
                with open(filename, "w", encoding="utf-8") as f:
                    json.dump(fall_system.alert_history, f, ensure_ascii=False, indent=2)
                return f"‚úÖ ƒê√£ xu·∫•t b√°o c√°o: {filename}"
            except Exception as e:
                return f"‚ùå L·ªói xu·∫•t b√°o c√°o: {e}"

        # Bind events
        start_btn.click(start_system, inputs=[camera_index], outputs=[control_output, status_display])

        stop_btn.click(stop_system, outputs=[control_output, status_display])

        refresh_logs_btn.click(lambda: fall_system.get_logs_display(), outputs=[logs_display])

        refresh_alerts_btn.click(lambda: fall_system.get_alert_history_display(), outputs=[alert_display])

        clear_alerts_btn.click(clear_alert_history, outputs=[control_output, alert_display])

        export_alerts_btn.click(export_alert_report, outputs=[control_output])

        # Video upload event handlers
        def process_video(video_file):
            if not video_file:
                return "‚ùå Ch∆∞a ch·ªçn video!", "Vui l√≤ng ch·ªçn file video"

            # Process video in a separate thread to avoid blocking UI
            def video_worker():
                return fall_system.process_uploaded_video(video_file)

            return video_worker()

        def refresh_evidence():
            display_text, evidence_choices, gif_paths = fall_system.get_evidence_gifs_display()
            return display_text, gr.update(choices=evidence_choices, value=None), "Ch·ªçn m·ªôt b·∫±ng ch·ª©ng t·ª´ dropdown ƒë·ªÉ xem chi ti·∫øt...", None

        def clear_evidence():
            try:
                # Clear from memory
                fall_system.evidence_gifs.clear()
                fall_system.add_log("üóëÔ∏è ƒê√£ x√≥a danh s√°ch b·∫±ng ch·ª©ng GIF", "info")
                display_text, evidence_choices, gif_paths = fall_system.get_evidence_gifs_display()
                return (
                    "‚úÖ ƒê√£ x√≥a danh s√°ch b·∫±ng ch·ª©ng!",
                    display_text,
                    gr.update(choices=evidence_choices, value=None),
                    "Ch·ªçn m·ªôt b·∫±ng ch·ª©ng t·ª´ dropdown ƒë·ªÉ xem chi ti·∫øt...",
                    None,
                )
            except Exception as e:
                return f"‚ùå L·ªói x√≥a b·∫±ng ch·ª©ng: {e}", "L·ªói", gr.update(choices=[], value=None), "L·ªói", None

        def on_evidence_select(selected_choice):
            if selected_choice is None:
                return "Ch·ªçn m·ªôt b·∫±ng ch·ª©ng t·ª´ dropdown ƒë·ªÉ xem chi ti·∫øt...", None

            # selected_choice is now just the text string
            details, gif_path = fall_system.get_evidence_details(selected_choice)
            return details, gif_path

        # Bind new events
        upload_btn.click(process_video, inputs=[video_upload], outputs=[upload_status, upload_info])

        refresh_evidence_btn.click(refresh_evidence, outputs=[evidence_summary, evidence_selector, evidence_details, evidence_gif_display])

        clear_evidence_btn.click(clear_evidence, outputs=[upload_status, evidence_summary, evidence_selector, evidence_details, evidence_gif_display])

        # Evidence selection event
        evidence_selector.change(on_evidence_select, inputs=[evidence_selector], outputs=[evidence_details, evidence_gif_display])

        # AI & Audio Control Event Handlers
        def on_detection_method_change(method):
            return fall_system.set_detection_method(method)

        def on_load_model():
            return fall_system.load_videollama3_model()

        def on_unload_model():
            return fall_system.unload_videollama3_model()

        def on_test_audio():
            return fall_system.test_audio_warning()

        def on_audio_enabled_change(enabled):
            return fall_system.set_audio_enabled(enabled)

        def on_audio_volume_change(volume):
            return fall_system.set_audio_volume(volume)

        # Bind AI & Audio events
        detection_method.change(on_detection_method_change, inputs=[detection_method], outputs=[method_output])

        load_model_btn.click(on_load_model, outputs=[model_output])

        unload_model_btn.click(on_unload_model, outputs=[model_output])

        test_audio_btn.click(on_test_audio, outputs=[audio_output])

        audio_enabled.change(on_audio_enabled_change, inputs=[audio_enabled], outputs=[audio_output])

        audio_volume.change(on_audio_volume_change, inputs=[audio_volume], outputs=[audio_output])

        # Faster refresh for camera feed (0.1s for real-time video)
        def update_camera():
            return fall_system.get_current_frame()

        # Enhanced status update with model status checking
        def update_status_and_logs_enhanced():
            # Check for UI updates from queue
            model_status_msg = ""
            try:
                while not fall_system.ui_update_queue.empty():
                    update_type, message = fall_system.ui_update_queue.get_nowait()
                    if update_type in ["model_loaded", "model_error", "model_unloaded"]:
                        model_status_msg = message
            except:
                pass

            if fall_system.is_running:
                evidence_text, evidence_choices, gif_paths = fall_system.get_evidence_gifs_display()
                if model_status_msg:
                    return (
                        fall_system.get_status_info(),
                        fall_system.get_logs_display(),
                        fall_system.get_alert_history_display(),
                        evidence_text,
                        gr.update(choices=evidence_choices),  # Only update choices, preserve value
                        gr.update(),  # evidence_details - don't change
                        gr.update(),  # evidence_gif_display - don't change
                        model_status_msg,
                    )
                else:
                    return (
                        fall_system.get_status_info(),
                        fall_system.get_logs_display(),
                        fall_system.get_alert_history_display(),
                        evidence_text,
                        gr.update(choices=evidence_choices),  # Only update choices, preserve value
                        gr.update(),  # evidence_details - don't change
                        gr.update(),  # evidence_gif_display - don't change
                        fall_system.get_model_status_message(),
                    )
            else:
                evidence_text, evidence_choices, gif_paths = fall_system.get_evidence_gifs_display()
                if model_status_msg:
                    return (
                        fall_system.get_status_info(),
                        gr.update(),
                        gr.update(),
                        evidence_text,
                        gr.update(choices=evidence_choices),
                        gr.update(),
                        gr.update(),
                        model_status_msg,
                    )
                else:
                    return (
                        fall_system.get_status_info(),
                        gr.update(),
                        gr.update(),
                        evidence_text,
                        gr.update(choices=evidence_choices),
                        gr.update(),
                        gr.update(),
                        fall_system.get_model_status_message(),
                    )

        # Set up dual auto-refresh timers using gr.Timer (Gradio 5.x)
        try:
            # Fast timer for camera feed (0.1s = 10 FPS)
            camera_timer = gr.Timer(0.1)
            camera_timer.tick(update_camera, outputs=[camera_feed])

            # Slower timer for status and logs (2s) with model status
            status_timer = gr.Timer(2.0)
            status_timer.tick(
                update_status_and_logs_enhanced,
                outputs=[status_display, logs_display, alert_display, evidence_summary, evidence_selector, evidence_details, evidence_gif_display, model_output],
            )

            print("‚úÖ Enhanced dual refresh timers set up: Camera 0.1s, Status/Logs 2s with model status")

        except Exception as e:
            print(f"‚ö†Ô∏è Enhanced auto-refresh timers not available: {e}")
            # Fallback: single timer for everything at 1s
            try:
                fallback_timer = gr.Timer(1.0)

                def update_all():
                    if fall_system.is_running:
                        evidence_text, evidence_choices, gif_paths = fall_system.get_evidence_gifs_display()
                        return (
                            fall_system.get_current_frame(),
                            fall_system.get_status_info(),
                            fall_system.get_logs_display(),
                            fall_system.get_alert_history_display(),
                            evidence_text,
                            gr.update(choices=evidence_choices),  # Only update choices, preserve value
                            gr.update(),  # evidence_details - don't change
                            gr.update(),  # evidence_gif_display - don't change
                            fall_system.get_model_status_message(),
                        )
                    else:
                        evidence_text, evidence_choices, gif_paths = fall_system.get_evidence_gifs_display()
                        return (
                            fall_system.get_current_frame(),
                            fall_system.get_status_info(),
                            gr.update(),
                            gr.update(),
                            evidence_text,
                            gr.update(choices=evidence_choices),  # Only update choices, preserve value
                            gr.update(),  # evidence_details - don't change
                            gr.update(),  # evidence_gif_display - don't change
                            fall_system.get_model_status_message(),
                        )

                fallback_timer.tick(
                    update_all,
                    outputs=[
                        camera_feed,
                        status_display,
                        logs_display,
                        alert_display,
                        evidence_summary,
                        evidence_selector,
                        evidence_details,
                        evidence_gif_display,
                        model_output,
                    ],
                )
                print("‚ö†Ô∏è Using enhanced fallback timer: All components 1s with model status")
            except:
                print("‚ùå No auto-refresh available - manual refresh only")

    return demo


if __name__ == "__main__":
    # Create and launch the interface
    demo = create_interface()

    print("üöÄ ƒêang kh·ªüi ƒë·ªông Gradio Web UI...")
    print("üì± Truy c·∫≠p giao di·ªán web t·∫°i: http://localhost:7860")
    print("üõë Nh·∫•n Ctrl+C ƒë·ªÉ d·ª´ng server")

    try:
        demo.launch(server_name="0.0.0.0", server_port=7860, share=True, show_error=True, quiet=False, prevent_thread_lock=False)  # Use localhost first
    except Exception as e:
        print(f"‚ùå Localhost launch failed: {e}")
        print("üîÑ Trying with network access...")
        try:
            demo.launch(server_name="0.0.0.0", server_port=7860, share=False, show_error=True, quiet=False, prevent_thread_lock=False)  # Allow external access
        except Exception as e2:
            print(f"‚ùå Network launch failed: {e2}")
            print("üåê Creating shareable link...")
            demo.launch(share=True, show_error=True, quiet=False, prevent_thread_lock=False)  # Create public link as last resort
