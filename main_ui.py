import asyncio
import json
import os
import queue
import threading
import time
from datetime import datetime

import cv2
import gradio as gr
import numpy as np
from PIL import Image

from src import (
    OPENAI_CLIENT,
    SAVE_ANALYSIS_FRAMES,
    SAVE_FORMAT,
    TELEGRAM_BOT,
    USE_TELE_ALERT,
    alert_services,
)
from src.audio_warning import AudioWarningSystem
from src.utils import frames_to_base64, prepare_messages, save_analysis_frames_to_temp
from src.videollama_detector import VideoLLamaFallDetector
from loguru import logger

class FallDetectionWebUI:
    def __init__(self):
        # Camera and detection settings
        self.camera = None
        self.is_running = False
        self.analysis_interval = 5
        self.frame_buffer = []
        self.last_analysis_time = 0
        self.fall_detected_cooldown = 30
        self.last_fall_alert = 0
        self.frame_count = 0
        self.analysis_count = 0
        self.start_time = time.time()

        # Detection method: "openai" or "videollama3"
        self.detection_method = "openai"

        # Initialize VideoLLaMA3 detector
        self.videollama_detector = VideoLLamaFallDetector()

        # Initialize audio warning system
        self.audio_warning = AudioWarningSystem()

        # UI specific
        self.current_frame = None
        self.alert_history = []
        self.system_logs = []
        self.status_data = {}
        self.ui_update_queue = queue.Queue()

        # Status tracking
        self.camera_status = "KhÃ´ng hoáº¡t Ä‘á»™ng"
        self.last_analysis_result = "ChÆ°a cÃ³ phÃ¢n tÃ­ch"

        # Video upload processing
        self.upload_processing = False
        self.uploaded_video_path = None

        # Evidence storage
        self.evidence_gifs = []  # Store paths to saved GIF evidence

    def initialize_camera(self, camera_index=0):
        """Initialize camera capture"""
        try:
            if self.camera:
                self.camera.release()

            self.camera = cv2.VideoCapture(camera_index)
            if not self.camera.isOpened():
                raise Exception(f"KhÃ´ng thá»ƒ má»Ÿ camera {camera_index}")

            # Set camera properties
            self.camera.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
            self.camera.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
            self.camera.set(cv2.CAP_PROP_FPS, 30)

            self.camera_status = "Hoáº¡t Ä‘á»™ng"
            self.add_log("âœ“ Camera Ä‘Ã£ Ä‘Æ°á»£c khá»Ÿi táº¡o thÃ nh cÃ´ng", "success")
            return True

        except Exception as e:
            self.camera_status = "Lá»—i"
            self.add_log(f"âœ— KhÃ´ng thá»ƒ khá»Ÿi táº¡o camera: {e}", "error")
            return False

    def add_log(self, message, log_type="info"):
        """Add log message with timestamp"""
        timestamp = datetime.now().strftime("%H:%M:%S")
        logger.info(f"[{timestamp}] {log_type} {message}")
        log_entry = {"time": timestamp, "message": message, "type": log_type}
        self.system_logs.append(log_entry)
        # Keep only last 100 logs
        if len(self.system_logs) > 100:
            self.system_logs.pop(0)

    def capture_frames(self):
        """Continuously capture frames from camera"""
        while self.is_running and self.camera:
            ret, frame = self.camera.read()
            if not ret:
                self.add_log("âš  KhÃ´ng thá»ƒ chá»¥p khung hÃ¬nh", "warning")
                continue

            # Add timestamp to frame
            timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            cv2.putText(frame, timestamp, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)

            # Store frame with timestamp
            current_time = time.time()
            self.frame_count += 1
            self.frame_buffer.append({"frame": frame, "timestamp": current_time})

            # Keep only recent frames (last 10 seconds)
            self.frame_buffer = [f for f in self.frame_buffer if current_time - f["timestamp"] < 10]

            # Update current frame for UI
            self.current_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

            # Check for analysis trigger
            if current_time - self.last_analysis_time >= self.analysis_interval:
                threading.Thread(target=self.analyze_frames, daemon=True).start()
                self.last_analysis_time = current_time

            time.sleep(0.03)  # ~30 FPS

    def analyze_frames(self):
        """Analyze recent frames for fall detection using selected method"""
        if not self.frame_buffer:
            return

        try:
            self.analysis_count += 1
            self.add_log(f"ğŸ” Báº¯t Ä‘áº§u phÃ¢n tÃ­ch láº§n {self.analysis_count}...", "info")

            # Get recent frames
            recent_frames = self.frame_buffer.copy()

            # Save analysis frames if enabled
            if SAVE_ANALYSIS_FRAMES:
                threading.Thread(target=save_analysis_frames_to_temp, args=([recent_frames])).start()

            # Choose analysis method
            if self.detection_method == "videollama3":
                analysis_result = self.analyze_frames_videollama3(recent_frames)
            else:  # Default to OpenAI
                analysis_result = self.analyze_frames_openai(recent_frames)

            if analysis_result:
                self.last_analysis_result = analysis_result
                self.add_log(f"ğŸ“Š Káº¿t quáº£ phÃ¢n tÃ­ch: {analysis_result}", "info")

                # Check for fall detection (Vietnamese)
                if analysis_result.startswith("PHÃT_HIá»†N_TÃ‰_NGÃƒ"):
                    self.handle_fall_detection(analysis_result)

        except Exception as e:
            self.add_log(f"âŒ Lá»—i phÃ¢n tÃ­ch: {e}", "error")

    def analyze_frames_openai(self, recent_frames):
        """Analyze frames using VLM SmolVLM"""
        try:
            base64_frames = frames_to_base64(recent_frames)
            if not base64_frames:
                return None

            # Call OpenAI API
            response = OPENAI_CLIENT.chat.completions.create(model="gpt-4o-mini", messages=prepare_messages(base64_frames), max_tokens=150)

            return response.choices[0].message.content.strip()
        except Exception as e:
            self.add_log(f"âŒ Lá»—i OpenAI API: {e}", "error")
            return None

    def analyze_frames_videollama3(self, recent_frames):
        """Analyze frames using local VideoLLaMA3 model + OpenAI Vietnamese analysis"""
        try:
            if not self.videollama_detector.is_loaded:
                self.add_log("âš ï¸ VideoLLaMA3 model chÆ°a Ä‘Æ°á»£c táº£i", "warning")
                return None

            # Check if OpenAI is available for Vietnamese analysis
            openai_available = bool(os.environ.get("OPENAI_API_KEY"))
            if not openai_available:
                self.add_log("âš ï¸ OpenAI API key khÃ´ng cÃ³, khÃ´ng thá»ƒ phÃ¢n tÃ­ch tiáº¿ng Viá»‡t", "warning")
                return "Lá»–I_Cáº¤U_HÃŒNH: Thiáº¿u OpenAI API key cho phÃ¢n tÃ­ch tiáº¿ng Viá»‡t"

            self.add_log("ğŸ”„ Báº¯t Ä‘áº§u quÃ¡ trÃ¬nh phÃ¢n tÃ­ch 2 bÆ°á»›c: VideoLLaMA3 â†’ OpenAI", "info")

            # Call the combined analysis method
            result = self.videollama_detector.analyze_frames(recent_frames)

            if result.startswith("Lá»–I_PHÃ‚N_TÃCH_Káº¾T_Há»¢P"):
                self.add_log(f"âŒ Lá»—i phÃ¢n tÃ­ch káº¿t há»£p: {result}", "error")
            elif result.startswith("PHÃT_HIá»†N_TÃ‰_NGÃƒ"):
                self.add_log("âœ… HoÃ n thÃ nh phÃ¢n tÃ­ch 2 bÆ°á»›c - PhÃ¡t hiá»‡n tÃ© ngÃ£!", "success")
            elif result.startswith("KHÃ”NG_PHÃT_HIá»†N_TÃ‰_NGÃƒ"):
                self.add_log("âœ… HoÃ n thÃ nh phÃ¢n tÃ­ch 2 bÆ°á»›c - KhÃ´ng cÃ³ tÃ© ngÃ£", "success")
            else:
                self.add_log("âš ï¸ Káº¿t quáº£ phÃ¢n tÃ­ch khÃ´ng theo Ä‘á»‹nh dáº¡ng mong Ä‘á»£i", "warning")

            return result

        except Exception as e:
            self.add_log(f"âŒ Lá»—i VideoLLaMA3 + OpenAI: {e}", "error")
            return None

    def handle_fall_detection(self, analysis_result):
        """Handle detected fall - send alerts and play audio warning"""
        current_time = time.time()

        # Check cooldown to prevent spam
        if current_time - self.last_fall_alert < self.fall_detected_cooldown:
            self.add_log("â³ PhÃ¡t hiá»‡n tÃ© ngÃ£ nhÆ°ng váº«n trong thá»i gian chá»", "warning")
            return

        self.last_fall_alert = current_time
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

        # Add to alert history
        alert_data = {
            "timestamp": timestamp,
            "details": analysis_result,
            "frame_count": len(self.frame_buffer),
            "evidence_saved": SAVE_ANALYSIS_FRAMES,
            "source": "Live Camera",
            "detection_method": self.detection_method.upper(),
        }
        self.alert_history.append(alert_data)

        # Log the alert
        self.add_log(f"ğŸš¨ PHÃT HIá»†N TÃ‰ NGÃƒ: {analysis_result}", "alert")

        # Play audio warning (async to avoid blocking)
        self.audio_warning.play_warning_async(analysis_result)
        self.add_log("ğŸ”Š ÄÃ£ phÃ¡t cáº£nh bÃ¡o Ã¢m thanh", "success")

        # Save evidence as GIF
        try:
            gif_folder = self.save_evidence_gif(self.frame_buffer, timestamp, "Live Camera")
            if gif_folder:
                alert_data["gif_evidence"] = gif_folder
                self.evidence_gifs.append(
                    {"path": gif_folder, "timestamp": timestamp, "source": "Live Camera", "details": analysis_result, "detection_method": self.detection_method.upper()}
                )
                self.add_log(f"ğŸ’¾ ÄÃ£ lÆ°u báº±ng chá»©ng GIF: {os.path.basename(gif_folder)}", "success")
        except Exception as e:
            self.add_log(f"âŒ Lá»—i lÆ°u GIF: {e}", "error")

        # Send Telegram notification only if enabled
        if TELEGRAM_BOT and USE_TELE_ALERT:
            asyncio.create_task(alert_services.send_telegram_alert(analysis_result, timestamp, self.frame_buffer))
            self.add_log("ğŸ“± ThÃ´ng bÃ¡o Telegram Ä‘Ã£ gá»­i", "success")
        else:
            self.add_log("â„¹ Bá» qua thÃ´ng bÃ¡o Telegram (Ä‘Ã£ táº¯t hoáº·c chÆ°a cáº¥u hÃ¬nh)", "info")

        # Save current frame as evidence (original format)
        if self.frame_buffer:
            threading.Thread(target=save_analysis_frames_to_temp, args=([self.frame_buffer])).start()

    def start_detection(self, camera_index):
        """Start the fall detection system"""
        if self.is_running:
            return "âŒ Há»‡ thá»‘ng Ä‘Ã£ Ä‘ang cháº¡y!", self.get_status_info()

        if not self.initialize_camera(camera_index):
            return "âŒ KhÃ´ng thá»ƒ khá»Ÿi táº¡o camera!", self.get_status_info()

        self.is_running = True
        self.start_time = time.time()
        self.add_log("ğŸš€ Há»‡ thá»‘ng phÃ¡t hiá»‡n tÃ© ngÃ£ Ä‘Ã£ khá»Ÿi Ä‘á»™ng", "success")

        # Start capture thread
        threading.Thread(target=self.capture_frames, daemon=True).start()

        return "âœ… Há»‡ thá»‘ng Ä‘Ã£ khá»Ÿi Ä‘á»™ng thÃ nh cÃ´ng!", self.get_status_info()

    def stop_detection(self):
        """Stop the fall detection system"""
        if not self.is_running:
            return "âŒ Há»‡ thá»‘ng chÆ°a cháº¡y!", self.get_status_info()

        self.is_running = False
        self.camera_status = "ÄÃ£ dá»«ng"

        if self.camera:
            self.camera.release()
            self.camera = None

        self.add_log("ğŸ›‘ Há»‡ thá»‘ng phÃ¡t hiá»‡n tÃ© ngÃ£ Ä‘Ã£ dá»«ng", "info")
        return "âœ… Há»‡ thá»‘ng Ä‘Ã£ dá»«ng!", self.get_status_info()

    def get_current_frame(self):
        """Get current frame for display"""
        if self.current_frame is not None:
            return Image.fromarray(self.current_frame)
        else:
            # Return a placeholder image
            placeholder = np.zeros((480, 640, 3), dtype=np.uint8)
            cv2.putText(placeholder, "Camera chua khoi dong", (150, 240), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
            return Image.fromarray(placeholder)

    def get_status_info(self):
        """Get system status information"""
        uptime = time.strftime("%H:%M:%S", time.gmtime(time.time() - self.start_time))

        # Get VideoLLaMA3 status
        llama_status = self.videollama_detector.get_model_status()
        audio_status = self.audio_warning.get_status()

        # Check OpenAI availability for VideoLLaMA3 method
        openai_available = bool(os.environ.get("OPENAI_API_KEY"))

        status_text = f"""
ğŸ“Š **TRáº NG THÃI Há»† THá»NG**

ğŸ¥ **Camera:** {self.camera_status}

ğŸ¤– **PhÆ°Æ¡ng thá»©c phÃ¡t hiá»‡n:** {"SmolVLM" if self.detection_method == "openai" else "VideoLLaMA3 + OpenAI"}

ğŸ” **Láº§n phÃ¢n tÃ­ch:** {self.analysis_count}

ğŸ“± **Gá»­i Tin Nháº¯n:** {'Báº­t' if USE_TELE_ALERT else 'Táº¯t'}

ğŸ’¾ **LÆ°u Frames:** {'Báº­t (' + SAVE_FORMAT.upper() + ')' if SAVE_ANALYSIS_FRAMES else 'Táº¯t'}

â° **Thá»i gian hoáº¡t Ä‘á»™ng:** {uptime}

ğŸ”„ **Chu ká»³:** {self.analysis_interval}s

ğŸ“ˆ **Khung hÃ¬nh/Buffer Frames:** {self.frame_count} / {len(self.frame_buffer)}

ğŸš¨ **Cáº£nh bÃ¡o:** {len(self.alert_history)}

ğŸ”Š **Audio Warning:** {'âœ… Enabled' if audio_status['enabled'] else 'âŒ Disabled'} ({audio_status['tts_method']})

ğŸ“‹ **Káº¿t quáº£ phÃ¢n tÃ­ch gáº§n nháº¥t:**
{self.last_analysis_result}
        """
        return status_text

    def get_logs_display(self):
        """Get formatted logs for display"""
        if not self.system_logs:
            return "ChÆ°a cÃ³ log nÃ o..."

        log_text = ""
        for log in self.system_logs[-30:]:  # Show last 30 logs
            emoji = {"info": "â„¹ï¸", "success": "âœ…", "warning": "âš ï¸", "error": "âŒ", "alert": "ğŸš¨"}
            icon = emoji.get(log["type"], "ğŸ“")
            log_text += f"[{log['time']}] {icon} {log['message']}\n"

        return log_text

    def get_alert_history_display(self):
        """Get formatted alert history for display"""
        if not self.alert_history:
            return "ChÆ°a cÃ³ cáº£nh bÃ¡o nÃ o..."

        alert_text = ""
        for i, alert in enumerate(reversed(self.alert_history[-10:])):  # Show last 10 alerts
            alert_text += f"""
**Cáº£nh bÃ¡o #{len(self.alert_history) - i}**
ğŸ• Thá»i gian: {alert['timestamp']}
ğŸ“ Chi tiáº¿t: {alert['details']}
ğŸ“Š Frames: {alert['frame_count']}
ğŸ’¾ Báº±ng chá»©ng: {'ÄÃ£ lÆ°u' if alert['evidence_saved'] else 'KhÃ´ng lÆ°u'}
---
            """

        return alert_text

    def process_uploaded_video(self, video_path):
        """Process uploaded video file for fall detection - analyze entire video as one piece"""
        if not video_path:
            return "âŒ KhÃ´ng cÃ³ video Ä‘Æ°á»£c upload!", "Vui lÃ²ng chá»n file video"

        if self.upload_processing:
            return "âŒ Äang xá»­ lÃ½ video khÃ¡c!", "Vui lÃ²ng chá» hoÃ n thÃ nh"

        self.upload_processing = True
        self.uploaded_video_path = video_path

        try:
            self.add_log(f"ğŸ“ Báº¯t Ä‘áº§u phÃ¢n tÃ­ch toÃ n bá»™ video: {os.path.basename(video_path)}", "info")

            # Open video file
            cap = cv2.VideoCapture(video_path)
            if not cap.isOpened():
                raise Exception("KhÃ´ng thá»ƒ má»Ÿ file video")

            # Get video properties
            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            fps = cap.get(cv2.CAP_PROP_FPS)
            duration = total_frames / fps if fps > 0 else 0

            self.add_log(f"ğŸ“Š Video info: {total_frames} frames, {fps:.1f} FPS, {duration:.1f}s", "info")

            # Read all frames for complete analysis
            frame_buffer = []
            frame_count = 0

            # Sample frames to avoid memory issues (max 60 frames for analysis)
            sample_interval = max(1, total_frames // 60) if total_frames > 60 else 1

            while True:
                ret, frame = cap.read()
                if not ret:
                    break

                # frame_count += 1
                # current_time = frame_count / fps if fps > 0 else frame_count * 0.033

                # # Sample frames at intervals to keep memory usage reasonable
                # if frame_count % sample_interval == 0:
                current_time = frame_count / fps if fps > 0 else frame_count * 0.033
                frame_buffer.append({"frame": frame, "timestamp": current_time})

                # Update progress

            cap.release()

            if not frame_buffer:
                raise Exception("KhÃ´ng thá»ƒ Ä‘á»c frame nÃ o tá»« video")

            self.add_log(f"ğŸ“Š ÄÃ£ sample {len(frame_buffer)} frames tá»« {frame_count} frames tá»•ng", "info")

            # Analyze the entire video as one piece
            self.add_log("ğŸ” Báº¯t Ä‘áº§u phÃ¢n tÃ­ch toÃ n bá»™ video...", "info")

            analysis_result = self.analyze_video_frames(frame_buffer, 1, video_path)

            # Display result prominently
            result_summary = ""
            if analysis_result:
                self.last_analysis_result = analysis_result
                self.add_log(f"ğŸ“Š Káº¿t quáº£ phÃ¢n tÃ­ch: {analysis_result}", "info")

                # Check for fall detection
                if "PHÃT_HIá»†N_TÃ‰_NGÃƒ" in analysis_result:
                    self.handle_video_fall_detection(analysis_result, frame_buffer, duration / 2, video_path)
                    result_summary = f"ğŸš¨ TÃ‰ NGÃƒ ÄÆ¯á»¢C PHÃT HIá»†N!\n{analysis_result}"
                elif "KHÃ”NG_PHÃT_HIá»†N_TÃ‰_NGÃƒ" in analysis_result:
                    result_summary = f"âœ… KHÃ”NG CÃ“ TÃ‰ NGÃƒ\n{analysis_result}"
                else:
                    result_summary = f"ğŸ“Š Káº¾T QUáº¢ PHÃ‚N TÃCH\n{analysis_result}"
            else:
                result_summary = "âŒ KhÃ´ng thá»ƒ phÃ¢n tÃ­ch video"

            self.upload_processing = False

            completion_msg = f"âœ… HoÃ n thÃ nh phÃ¢n tÃ­ch video!\nFrames gá»‘c: {frame_count}\nFrames phÃ¢n tÃ­ch: {len(frame_buffer)}"
            self.add_log(completion_msg, "success")

            video_info = f"""ğŸ“¹ Video: {os.path.basename(video_path)}
â±ï¸ Thá»i lÆ°á»£ng: {duration:.1f}s
ğŸ“Š Frames: {frame_count} (phÃ¢n tÃ­ch {len(frame_buffer)})
ğŸ¤– PhÆ°Æ¡ng thá»©c: {self.detection_method.upper()}

{result_summary}"""

            return completion_msg, video_info

        except Exception as e:
            self.upload_processing = False
            error_msg = f"âŒ Lá»—i xá»­ lÃ½ video: {e}"
            self.add_log(error_msg, "error")
            return error_msg, f"Xá»­ lÃ½ tháº¥t báº¡i: {str(e)}"

    def analyze_video_frames(self, frame_buffer, analysis_count, source_video):
        """Analyze frames from uploaded video"""
        if not frame_buffer:
            return None

        try:
            self.add_log(f"ğŸ” PhÃ¢n tÃ­ch video hoÃ n chá»‰nh ({len(frame_buffer)} frames) - {self.detection_method.upper()}...", "info")

            # Choose analysis method
            if self.detection_method == "videollama3":
                if not self.videollama_detector.is_loaded:
                    self.add_log("âš ï¸ VideoLLaMA3 model chÆ°a Ä‘Æ°á»£c táº£i, chuyá»ƒn vá» OpenAI", "warning")
                    return self.analyze_video_frames_openai(frame_buffer)
                else:
                    return self.videollama_detector.analyze_frames(frame_buffer)
            else:
                return self.analyze_video_frames_openai(frame_buffer)

        except Exception as e:
            self.add_log(f"âŒ Lá»—i phÃ¢n tÃ­ch video frames: {e}", "error")
            return None

    def analyze_video_frames_openai(self, frame_buffer):
        """Analyze video frames using OpenAI"""
        # Sample frames to avoid too many (max 8 frames for better analysis)
        if len(frame_buffer) > 16:
            step = len(frame_buffer) // 16
            sample_frames = frame_buffer[::step][:16]  # Take exactly 8 frames
        else:
            sample_frames = frame_buffer

        base64_frames = frames_to_base64(sample_frames)

        if not base64_frames:
            return None

        self.add_log(f"ğŸ“¤ Gá»­i {len(base64_frames)} frames tá»›i OpenAI Ä‘á»ƒ phÃ¢n tÃ­ch...", "info")

        # Call OpenAI API
        response = OPENAI_CLIENT.chat.completions.create(model="gpt-4o-mini", messages=prepare_messages(base64_frames), max_tokens=150)

        analysis_result = response.choices[0].message.content.strip()
        self.add_log(f"ğŸ“Š Káº¿t quáº£ phÃ¢n tÃ­ch OpenAI: {analysis_result}", "info")
        return analysis_result

    def handle_video_fall_detection(self, analysis_result, frame_buffer, timestamp, source_video):
        """Handle fall detection from uploaded video"""
        detection_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

        # Create alert data for video
        alert_data = {
            "timestamp": detection_time,
            "details": analysis_result,
            "frame_count": len(frame_buffer),
            "evidence_saved": True,
            "source": f"Video: {os.path.basename(source_video)}",
            "video_timestamp": f"{timestamp:.1f}s",
            "detection_method": self.detection_method.upper(),
        }

        self.alert_history.append(alert_data)
        self.add_log(f"ğŸš¨ PHÃT HIá»†N TÃ‰ NGÃƒ TRONG VIDEO: {analysis_result} táº¡i {timestamp:.1f}s", "alert")

        # Play audio warning for video detection too
        self.audio_warning.play_warning_async(analysis_result)
        self.add_log("ğŸ”Š ÄÃ£ phÃ¡t cáº£nh bÃ¡o Ã¢m thanh cho video", "success")

        # Save evidence as GIF
        try:
            gif_folder = self.save_evidence_gif(frame_buffer, detection_time, source_video)
            if gif_folder:
                alert_data["gif_evidence"] = gif_folder
                self.evidence_gifs.append(
                    {
                        "path": gif_folder,
                        "timestamp": detection_time,
                        "source": os.path.basename(source_video),
                        "details": analysis_result,
                        "detection_method": self.detection_method.upper(),
                    }
                )
                self.add_log(f"ğŸ’¾ ÄÃ£ lÆ°u báº±ng chá»©ng GIF: {os.path.basename(gif_folder)}", "success")
        except Exception as e:
            self.add_log(f"âŒ Lá»—i lÆ°u GIF: {e}", "error")

        # Send Telegram notification if enabled
        if TELEGRAM_BOT and USE_TELE_ALERT:
            try:
                asyncio.create_task(
                    alert_services.send_telegram_alert(
                        f"ğŸ¬ {analysis_result} (Video: {os.path.basename(source_video)} táº¡i {timestamp:.1f}s)", detection_time, frame_buffer
                    )
                )
                self.add_log("ğŸ“± ThÃ´ng bÃ¡o Telegram Ä‘Ã£ gá»­i", "success")
            except Exception as e:
                self.add_log(f"âŒ Lá»—i gá»­i Telegram: {e}", "error")

    def save_evidence_gif(self, frame_buffer, timestamp, source_info):
        """Save frame buffer as GIF evidence with same format as temp folder"""
        try:
            # Create evidence directory if not exists
            evidence_dir = "evidence_gifs"
            os.makedirs(evidence_dir, exist_ok=True)

            # Create timestamp folder (matching temp folder format)
            safe_timestamp = timestamp.replace(":", "").replace(" ", "_").replace("-", "")
            timestamp_folder = os.path.join(evidence_dir, f"fall_{safe_timestamp}")
            os.makedirs(timestamp_folder, exist_ok=True)

            # Generate GIF filename
            gif_filename = "fall_evidence.gif"
            gif_path = os.path.join(timestamp_folder, gif_filename)

            # Generate info filename
            info_filename = "fall_info.txt"
            info_path = os.path.join(timestamp_folder, info_filename)

            # Convert frames to PIL Images with proper timing
            pil_frames = []
            frame_timestamps = []

            for i, frame_data in enumerate(frame_buffer):
                frame = frame_data["frame"]
                # Resize frame for consistent size (matching temp folder)
                height, width = frame.shape[:2]
                new_width = min(640, width)  # Max width 640px
                new_height = int(height * (new_width / width))
                resized_frame = cv2.resize(frame, (new_width, new_height))

                # Convert BGR to RGB
                pil_frame = Image.fromarray(cv2.cvtColor(resized_frame, cv2.COLOR_BGR2RGB))
                pil_frames.append(pil_frame)
                frame_timestamps.append(frame_data.get("timestamp", i * 0.1))

            # Save as GIF with proper timing (50 FPS for 5x speed)
            if pil_frames:
                duration_ms = 20  # 20ms per frame = 50 FPS (5x faster than before)
                pil_frames[0].save(gif_path, save_all=True, append_images=pil_frames[1:], duration=duration_ms, loop=0)

                # Create info file (matching temp folder format)
                gif_duration = len(pil_frames) * 0.02  # 50 FPS
                info_content = f"""Fall Detection Evidence
========================================
Timestamp: {timestamp}
Source: {source_info}
Detection Method: {self.detection_method.upper()}
Total frames: {len(pil_frames)}
Save format: gif
GIF duration: {gif_duration:.1f}s (50 FPS - 5x speed)

Files saved:
- {gif_filename}

Analysis Result:
{self.last_analysis_result}
"""

                with open(info_path, "w", encoding="utf-8") as f:
                    f.write(info_content)

                # Return the folder path (not just GIF path) for consistency
                return timestamp_folder

        except Exception as e:
            self.add_log(f"âŒ Lá»—i táº¡o GIF evidence: {e}", "error")
            return None

    def get_evidence_gifs_display(self):
        """Get list of evidence GIFs for display"""
        if not self.evidence_gifs:
            return "ChÆ°a cÃ³ báº±ng chá»©ng GIF nÃ o...", [], []

        # Create list of evidence items for selection
        evidence_choices = []
        gif_paths = []

        for i, evidence in enumerate(reversed(self.evidence_gifs)):  # Most recent first
            evidence_index = len(self.evidence_gifs) - i
            choice_text = f"#{evidence_index}: {evidence['timestamp']} - {evidence['source']}"
            evidence_choices.append(choice_text)  # Just the text, not tuple

            # Look for GIF file in the evidence folder
            evidence_folder = evidence["path"]
            if os.path.isdir(evidence_folder):
                gif_file = os.path.join(evidence_folder, "fall_evidence.gif")
                if os.path.exists(gif_file):
                    gif_paths.append(gif_file)
            elif evidence_folder.endswith(".gif") and os.path.exists(evidence_folder):
                gif_paths.append(evidence_folder)

        summary_text = f"ğŸ“ **CÃ“ {len(self.evidence_gifs)} Báº°NG CHá»¨NG GIF**\n\nChá»n má»™t má»¥c tá»« danh sÃ¡ch bÃªn dÆ°á»›i Ä‘á»ƒ xem chi tiáº¿t."

        return summary_text, evidence_choices, gif_paths

    def get_evidence_details(self, selected_choice):
        """Get detailed information for selected evidence"""
        if not selected_choice or not self.evidence_gifs:
            return "KhÃ´ng cÃ³ thÃ´ng tin", None

        try:
            # Extract the evidence number from the choice text (e.g., "#1: ..." -> 1)
            if selected_choice.startswith("#"):
                evidence_number = int(selected_choice.split(":")[0][1:])
                # Convert to index (evidence_number is 1-based, we need 0-based index from reversed list)
                selected_index = len(self.evidence_gifs) - evidence_number

                if selected_index < 0 or selected_index >= len(self.evidence_gifs):
                    return "Chá»‰ sá»‘ khÃ´ng há»£p lá»‡", None

                evidence = self.evidence_gifs[selected_index]
            else:
                return "Äá»‹nh dáº¡ng khÃ´ng há»£p lá»‡", None

        except (ValueError, IndexError):
            return "Lá»—i phÃ¢n tÃ­ch lá»±a chá»n", None

        details_text = f"""
**ğŸ¬ Báº°NG CHá»¨NG #{evidence_number}**

ğŸ“… **Thá»i gian:** {evidence['timestamp']}
ğŸ“ **Nguá»“n:** {evidence['source']}
ğŸ¤– **PhÆ°Æ¡ng thá»©c:** {evidence.get('detection_method', 'UNKNOWN')}
ğŸ“ **Chi tiáº¿t:** {evidence['details']}
ğŸ“„ **ThÆ° má»¥c:** {os.path.basename(evidence['path'])}

---
âœ¨ **GIF Ä‘Æ°á»£c tÄƒng tá»‘c 5x Ä‘á»ƒ xem nhanh hÆ¡n**
ğŸ¯ **PhÃ¢n tÃ­ch:** {evidence['details']}
        """

        # Get GIF path
        evidence_folder = evidence["path"]
        gif_path = None
        if os.path.isdir(evidence_folder):
            gif_file = os.path.join(evidence_folder, "fall_evidence.gif")
            if os.path.exists(gif_file):
                gif_path = gif_file
        elif evidence_folder.endswith(".gif") and os.path.exists(evidence_folder):
            gif_path = evidence_folder

        return details_text, gif_path

    def get_upload_progress(self):
        """Get current upload processing progress"""
        if not self.upload_processing:
            return "Sáºµn sÃ ng upload video", 0
        return f"Äang xá»­ lÃ½ video... {self.upload_progress}%", self.upload_progress

    def set_detection_method(self, method):
        """Set detection method (openai or videollama3)"""
        if method in ["openai", "videollama3"]:
            self.detection_method = method
            self.add_log(f"ğŸ”§ ÄÃ£ chuyá»ƒn phÆ°Æ¡ng thá»©c phÃ¡t hiá»‡n: {method.upper()}", "info")
            return f"âœ… ÄÃ£ chuyá»ƒn sang {method.upper()}"
        else:
            return "âŒ PhÆ°Æ¡ng thá»©c khÃ´ng há»£p lá»‡"

    def load_videollama3_model(self):
        """Load VideoLLaMA3 model"""
        if self.videollama_detector.is_loaded:
            return "âš ï¸ Model Ä‘Ã£ Ä‘Æ°á»£c táº£i rá»“i"

        self.add_log("ğŸš€ Äang táº£i VideoLLaMA3 model...", "info")

        def load_worker():
            success = self.videollama_detector.load_model()
            if success:
                self.add_log("âœ… VideoLLaMA3 model Ä‘Ã£ táº£i thÃ nh cÃ´ng", "success")
                # Force UI update by updating the queue
                self.ui_update_queue.put(("model_loaded", "âœ… VideoLLaMA3 model Ä‘Ã£ táº£i thÃ nh cÃ´ng"))
            else:
                self.add_log("âŒ KhÃ´ng thá»ƒ táº£i VideoLLaMA3 model", "error")
                self.ui_update_queue.put(("model_error", "âŒ KhÃ´ng thá»ƒ táº£i VideoLLaMA3 model"))

        threading.Thread(target=load_worker, daemon=True).start()
        return "â³ Äang táº£i model, vui lÃ²ng chá»..."

    def unload_videollama3_model(self):
        """Unload VideoLLaMA3 model"""
        if not self.videollama_detector.is_loaded:
            return "âš ï¸ Model chÆ°a Ä‘Æ°á»£c táº£i"

        self.videollama_detector.unload_model()
        self.add_log("ğŸ—‘ï¸ ÄÃ£ gá»¡ VideoLLaMA3 model", "info")
        self.ui_update_queue.put(("model_unloaded", "âœ… ÄÃ£ gá»¡ model khá»i bá»™ nhá»›"))
        return "âœ… ÄÃ£ gá»¡ model khá»i bá»™ nhá»›"

    def get_model_status_message(self):
        """Get current model status for UI display"""
        if self.videollama_detector.is_loaded:
            return "âœ… VideoLLaMA3 model Ä‘Ã£ sáºµn sÃ ng"
        else:
            return "âŒ VideoLLaMA3 model chÆ°a Ä‘Æ°á»£c táº£i"

    def test_audio_warning(self):
        """Test audio warning system"""
        result = self.audio_warning.test_audio_system()
        self.add_log(f"ğŸ”Š Test audio: {result}", "info")
        return result

    def set_audio_enabled(self, enabled):
        """Enable/disable audio warnings"""
        self.audio_warning.set_enabled(enabled)
        status = "báº­t" if enabled else "táº¯t"
        self.add_log(f"ğŸ”Š Cáº£nh bÃ¡o Ã¢m thanh Ä‘Ã£ {status}", "info")
        return f"âœ… Cáº£nh bÃ¡o Ã¢m thanh Ä‘Ã£ {status}"

    def set_audio_volume(self, volume):
        """Set audio volume"""
        self.audio_warning.set_volume(volume / 100.0)  # Convert from percentage
        self.add_log(f"ğŸ”Š Ã‚m lÆ°á»£ng Ä‘Ã£ Ä‘áº·t: {volume}%", "info")
        return f"âœ… Ã‚m lÆ°á»£ng: {volume}%"


# Initialize the system
fall_system = FallDetectionWebUI()


def create_interface():
    """Create the Gradio interface"""

    with gr.Blocks(
        title="ğŸ¥ Há»‡ Thá»‘ng PhÃ¡t Hiá»‡n TÃ© NgÃ£",
        theme=gr.themes.Soft(),
        css="""
        .alert-box { background-color: #ffebee; border-left: 4px solid #f44336; padding: 10px; }
        .success-box { background-color: #e8f5e8; border-left: 4px solid #4caf50; padding: 10px; }
        .info-box { background-color: #e3f2fd; border-left: 4px solid #2196f3; padding: 10px; }
        """,
    ) as demo:

        gr.HTML(
            """
        <div style="text-align: center; margin-bottom: 20px;">
            <h1>ğŸ¥ Há»† THá»NG PHÃT HIá»†N HÃ€NH VI NGÃƒ Cá»¦A CON NGÆ¯á»œI Báº°NG VLM</h1>
            <p style="font-size: 16px; color: #666;">
                Há»‡ thá»‘ng AI phÃ¡t hiá»‡n tÃ© ngÃ£ thá»i gian thá»±c sá»­ dá»¥ng Vision Language Model
            </p>
        </div>
        """
        )

        with gr.Tab("ğŸ¥ Camera & Äiá»u Khiá»ƒn"):
            with gr.Row():
                with gr.Column(scale=2):
                    camera_feed = gr.Image(label="ğŸ“¹ Camera Feed", type="pil", height=400, show_label=True)

                    with gr.Row():
                        camera_index = gr.Number(label="Chá»‰ sá»‘ Camera", value=0, precision=0, minimum=0, maximum=10)
                        start_btn = gr.Button("ğŸš€ Khá»Ÿi Äá»™ng", variant="primary", size="lg")
                        stop_btn = gr.Button("ğŸ›‘ Dá»«ng", variant="secondary", size="lg")

                with gr.Column(scale=1):
                    status_display = gr.Markdown(label="ğŸ“Š Tráº¡ng ThÃ¡i Há»‡ Thá»‘ng", value=fall_system.get_status_info())

                    control_output = gr.Textbox(label="ğŸ“¢ ThÃ´ng BÃ¡o Há»‡ Thá»‘ng", value="Sáºµn sÃ ng khá»Ÿi Ä‘á»™ng...", interactive=False)

        with gr.Tab("ğŸ“‹ Nháº­t KÃ½ Há»‡ Thá»‘ng"):
            logs_display = gr.Textbox(label="ğŸ“ System Logs (30 má»¥c gáº§n nháº¥t)", value=fall_system.get_logs_display(), lines=30, interactive=False, max_lines=25)

            refresh_logs_btn = gr.Button("ğŸ”„ Cáº­p Nháº­t Logs", size="sm")

        with gr.Tab("ğŸš¨ Lá»‹ch Sá»­ Cáº£nh BÃ¡o"):
            alert_display = gr.Markdown(label="ğŸš¨ Lá»‹ch Sá»­ TÃ© NgÃ£ (10 cáº£nh bÃ¡o gáº§n nháº¥t)", value=fall_system.get_alert_history_display())

            refresh_alerts_btn = gr.Button("ğŸ”„ Cáº­p Nháº­t Cáº£nh BÃ¡o", size="sm")

            with gr.Row():
                clear_alerts_btn = gr.Button("ğŸ—‘ï¸ XÃ³a Lá»‹ch Sá»­", variant="secondary")
                export_alerts_btn = gr.Button("ğŸ“ Xuáº¥t BÃ¡o CÃ¡o", variant="primary")

        with gr.Tab("ğŸ“ Upload Video"):
            gr.HTML(
                """
            <div class="info-box">
                <h3>ğŸ“ PhÃ¢n TÃ­ch Video Tá»« File</h3>
                <p>Upload video tá»« thiáº¿t bá»‹ Ä‘á»ƒ phÃ¢n tÃ­ch tÃ© ngÃ£. Há»— trá»£ cÃ¡c Ä‘á»‹nh dáº¡ng: MP4, AVI, MOV, MKV</p>
            </div>
            """
            )

            with gr.Row():
                with gr.Column(scale=2):
                    video_upload = gr.File(label="ğŸ“¹ Chá»n File Video", file_types=["video"], type="filepath")

                    upload_btn = gr.Button("ğŸ” PhÃ¢n TÃ­ch Video", variant="primary", size="lg")

                    gr.Progress()
                    upload_status = gr.Textbox(label="ğŸ“Š Tráº¡ng ThÃ¡i Xá»­ LÃ½", value="ChÆ°a cÃ³ video Ä‘Æ°á»£c upload...", interactive=False)

                with gr.Column(scale=1):
                    upload_info = gr.Textbox(label="â„¹ï¸ ThÃ´ng Tin Video", value="ChÆ°a chá»n video...", interactive=False, lines=10)

            gr.HTML(
                """
            <div class="alert-box">
                <h4>âš ï¸ LÆ°u Ã Quan Trá»ng</h4>
                <ul>
                    <li>Video sáº½ Ä‘Æ°á»£c phÃ¢n tÃ­ch toÃ n bá»™ nhÆ° má»™t Ä‘oáº¡n duy nháº¥t</li>
                    <li>Há»‡ thá»‘ng sáº½ sample tá»‘i Ä‘a 60 frames Ä‘á»ƒ phÃ¢n tÃ­ch (trÃ¡nh quÃ¡ táº£i bá»™ nhá»›)</li>
                    <li>Káº¿t quáº£ sáº½ hiá»ƒn thá»‹ ngay trong pháº§n thÃ´ng tin video</li>
                    <li>GIF báº±ng chá»©ng sáº½ Ä‘Æ°á»£c tá»± Ä‘á»™ng táº¡o khi phÃ¡t hiá»‡n tÃ© ngÃ£</li>
                    <li>Thá»i gian xá»­ lÃ½ phá»¥ thuá»™c vÃ o Ä‘á»™ dÃ i video vÃ  phÆ°Æ¡ng thá»©c phÃ¡t hiá»‡n</li>
                </ul>
            </div>
            """
            )

        with gr.Tab("ğŸ¬ Báº±ng Chá»©ng GIF"):
            gr.HTML(
                """
            <div class="success-box">
                <h3>ğŸ¬ Báº±ng Chá»©ng TÃ© NgÃ£ (GIF)</h3>
                <p>Xem cÃ¡c GIF báº±ng chá»©ng Ä‘Ã£ Ä‘Æ°á»£c lÆ°u khi phÃ¡t hiá»‡n tÃ© ngÃ£ - TÄƒng tá»‘c 5x</p>
            </div>
            """
            )

            with gr.Row():
                with gr.Column(scale=1):
                    evidence_summary = gr.Markdown(label="ğŸ“‹ Tá»•ng Quan", value=fall_system.get_evidence_gifs_display()[0])

                    evidence_selector = gr.Dropdown(
                        label="ğŸ¯ Chá»n Báº±ng Chá»©ng", choices=fall_system.get_evidence_gifs_display()[1], value=None, info="Chá»n má»™t báº±ng chá»©ng Ä‘á»ƒ xem chi tiáº¿t"
                    )

                    with gr.Row():
                        refresh_evidence_btn = gr.Button("ğŸ”„ Cáº­p Nháº­t", size="sm")
                        clear_evidence_btn = gr.Button("ğŸ—‘ï¸ XÃ³a Táº¥t Cáº£", variant="secondary", size="sm")

                with gr.Column(scale=2):
                    evidence_details = gr.Markdown(label="ğŸ“ Chi Tiáº¿t Báº±ng Chá»©ng", value="Chá»n má»™t báº±ng chá»©ng tá»« danh sÃ¡ch Ä‘á»ƒ xem chi tiáº¿t...")

                    evidence_gif_display = gr.Image(label="ğŸ¬ GIF Báº±ng Chá»©ng (5x Speed)", type="filepath", value=None, height=400)

            gr.HTML(
                """
            <div class="info-box">
                <h4>ğŸ“‹ HÆ°á»›ng Dáº«n Sá»­ Dá»¥ng</h4>
                <ul>
                    <li>ğŸ¯ Chá»n báº±ng chá»©ng tá»« dropdown Ä‘á»ƒ xem chi tiáº¿t</li>
                    <li>ğŸ¬ GIF Ä‘Æ°á»£c tÄƒng tá»‘c 5x Ä‘á»ƒ xem nhanh hÆ¡n</li>
                    <li>ğŸ“± GIF tá»± Ä‘á»™ng táº¡o khi phÃ¡t hiá»‡n tÃ© ngÃ£</li>
                    <li>ğŸ’¾ GIF lÆ°u trong thÆ° má»¥c <code>evidence_gifs/</code></li>
                    <li>ğŸ”„ Nháº¥n "Cáº­p Nháº­t" Ä‘á»ƒ lÃ m má»›i danh sÃ¡ch</li>
                </ul>
            </div>
            """
            )

        with gr.Tab("ğŸ¤– Cáº¥u HÃ¬nh AI & Ã‚m Thanh"):
            gr.HTML(
                """
            <div class="info-box">
                <h3>ğŸ¤– Quáº£n LÃ½ MÃ´ HÃ¬nh AI</h3>
                <p>Chá»n phÆ°Æ¡ng thá»©c phÃ¡t hiá»‡n tÃ© ngÃ£ vÃ  quáº£n lÃ½ mÃ´ hÃ¬nh VideoLLaMA3 local</p>
            </div>
            """
            )

            with gr.Row():
                with gr.Column(scale=1):
                    gr.Markdown("### ğŸ§  PhÆ°Æ¡ng Thá»©c PhÃ¡t Hiá»‡n")

                    detection_method = gr.Radio(
                        choices=[("VLM SmolVLM", "openai"), ("VideoLLaMA3 + OpenAI", "videollama3")],
                        value="openai",
                        label="Chá»n phÆ°Æ¡ng thá»©c phÃ¡t hiá»‡n",
                        info="SmolVLM: Trá»±c tiáº¿p phÃ¢n tÃ­ch. VideoLLaMA3: MÃ´ táº£ video â†’ OpenAI phÃ¢n tÃ­ch tiáº¿ng Viá»‡t",
                    )

                    method_output = gr.Textbox(label="ğŸ“¢ Tráº¡ng ThÃ¡i PhÆ°Æ¡ng Thá»©c", interactive=False)

                    gr.Markdown("### ğŸ¯ VideoLLaMA3 Model")

                    with gr.Row():
                        load_model_btn = gr.Button("ğŸ“¥ Táº£i Model", variant="primary")
                        unload_model_btn = gr.Button("ğŸ—‘ï¸ Gá»¡ Model", variant="secondary")

                    model_output = gr.Textbox(label="ğŸ“¢ Tráº¡ng ThÃ¡i Model", interactive=False)

                with gr.Column(scale=1):
                    gr.Markdown("### ğŸ”Š Cáº£nh BÃ¡o Ã‚m Thanh")

                    audio_enabled = gr.Checkbox(label="ğŸ”Š Báº­t cáº£nh bÃ¡o Ã¢m thanh", value=True, info="Tá»± Ä‘á»™ng phÃ¡t cáº£nh bÃ¡o khi phÃ¡t hiá»‡n tÃ© ngÃ£")

                    audio_volume = gr.Slider(minimum=0, maximum=100, value=80, step=5, label="ğŸ”‰ Ã‚m lÆ°á»£ng (%)", info="Äiá»u chá»‰nh Ã¢m lÆ°á»£ng cáº£nh bÃ¡o")

                    with gr.Row():
                        test_audio_btn = gr.Button("ğŸ”Š Test Ã‚m Thanh", variant="primary")
                        audio_output = gr.Textbox(label="ğŸ“¢ Tráº¡ng ThÃ¡i Audio", interactive=False)

            gr.HTML(
                """
            <div class="alert-box">
                <h4>âš ï¸ LÆ°u Ã Quan Trá»ng</h4>
                <ul>
                    <li><strong>OpenAI:</strong> Cáº§n API key vÃ  káº¿t ná»‘i internet, tá»‘c Ä‘á»™ phÃ¢n tÃ­ch nhanh</li>
                    <li><strong>VideoLLaMA3:</strong> PhÃ¢n tÃ­ch 2 bÆ°á»›c - VideoLLaMA3 mÃ´ táº£ video (offline) â†’ OpenAI phÃ¢n tÃ­ch tiáº¿ng Viá»‡t (online)</li>
                    <li><strong>YÃªu cáº§u VideoLLaMA3:</strong> Cáº§n cáº£ VideoLLaMA3 model VÃ€ OpenAI API key Ä‘á»ƒ hoáº¡t Ä‘á»™ng</li>
                    <li><strong>Audio:</strong> Cáº§n cÃ i Ä‘áº·t espeak-ng: <code>sudo apt-get install espeak-ng</code></li>
                    <li><strong>RAM:</strong> VideoLLaMA3 cáº§n ~4-8GB VRAM Ä‘á»ƒ cháº¡y mÆ°á»£t</li>
                </ul>
            </div>
            """
            )

        with gr.Tab("âš™ï¸ Cáº¥u HÃ¬nh"):
            gr.HTML(
                """
            <div class="info-box">
                <h3>ğŸ”§ Cáº¥u HÃ¬nh Há»‡ Thá»‘ng</h3>
                <p>CÃ¡c cáº¥u hÃ¬nh Ä‘Æ°á»£c Ä‘á»c tá»« file <code>.env</code>. Sau khi thay Ä‘á»•i, vui lÃ²ng khá»Ÿi Ä‘á»™ng láº¡i á»©ng dá»¥ng.</p>
            </div>
            """
            )

            with gr.Row():
                with gr.Column():
                    gr.Markdown(
                        """

                    ### ğŸ“± Cáº¥u HÃ¬nh Telegram
                    - **USE_TELE_ALERT**: true/false (máº·c Ä‘á»‹nh: false)
                    - **TELEGRAM_BOT_TOKEN**: Token bot Telegram
                    - **TELEGRAM_CHAT_ID**: ID chat nháº­n thÃ´ng bÃ¡o

                    ### ğŸ’¾ Cáº¥u HÃ¬nh LÆ°u Trá»¯
                    - **SAVE_ANALYSIS_FRAMES**: true/false (máº·c Ä‘á»‹nh: false)
                    - **SAVE_FORMAT**: images/gif/video/all (máº·c Ä‘á»‹nh: images)
                    - **MAX_FRAMES**: Sá»‘ frame tá»‘i Ä‘a gá»­i AI (máº·c Ä‘á»‹nh: 5)
                    """
                    )

                with gr.Column():
                    current_config = f"""
### ğŸ“Š Cáº¥u HÃ¬nh Hiá»‡n Táº¡i
- **Telegram**: {'âœ… Báº­t' if USE_TELE_ALERT else 'âŒ Táº¯t'}
- **LÆ°u Frames**: {'âœ… Báº­t' if SAVE_ANALYSIS_FRAMES else 'âŒ Táº¯t'}
- **Äá»‹nh Dáº¡ng**: {SAVE_FORMAT.upper()}
- **Max Frames**: {os.environ.get('MAX_FRAMES', 5)}
                    """
                    gr.Markdown(current_config)

        # Event handlers
        def start_system(camera_idx):
            message, status = fall_system.start_detection(int(camera_idx))
            return message, status

        def stop_system():
            message, status = fall_system.stop_detection()
            return message, status

        def clear_alert_history():
            fall_system.alert_history.clear()
            fall_system.add_log("ğŸ—‘ï¸ ÄÃ£ xÃ³a lá»‹ch sá»­ cáº£nh bÃ¡o", "info")
            return "âœ… ÄÃ£ xÃ³a lá»‹ch sá»­ cáº£nh bÃ¡o!", fall_system.get_alert_history_display()

        def export_alert_report():
            if not fall_system.alert_history:
                return "âŒ KhÃ´ng cÃ³ dá»¯ liá»‡u Ä‘á»ƒ xuáº¥t!"

            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"alert_report_{timestamp}.json"

            try:
                with open(filename, "w", encoding="utf-8") as f:
                    json.dump(fall_system.alert_history, f, ensure_ascii=False, indent=2)
                return f"âœ… ÄÃ£ xuáº¥t bÃ¡o cÃ¡o: {filename}"
            except Exception as e:
                return f"âŒ Lá»—i xuáº¥t bÃ¡o cÃ¡o: {e}"

        # Bind events
        start_btn.click(start_system, inputs=[camera_index], outputs=[control_output, status_display])

        stop_btn.click(stop_system, outputs=[control_output, status_display])

        refresh_logs_btn.click(lambda: fall_system.get_logs_display(), outputs=[logs_display])

        refresh_alerts_btn.click(lambda: fall_system.get_alert_history_display(), outputs=[alert_display])

        clear_alerts_btn.click(clear_alert_history, outputs=[control_output, alert_display])

        export_alerts_btn.click(export_alert_report, outputs=[control_output])

        # Video upload event handlers
        def process_video(video_file):
            if not video_file:
                return "âŒ ChÆ°a chá»n video!", "Vui lÃ²ng chá»n file video"

            # Process video in a separate thread to avoid blocking UI
            def video_worker():
                return fall_system.process_uploaded_video(video_file)

            return video_worker()

        def refresh_evidence():
            display_text, evidence_choices, gif_paths = fall_system.get_evidence_gifs_display()
            return display_text, gr.update(choices=evidence_choices, value=None), "Chá»n má»™t báº±ng chá»©ng tá»« dropdown Ä‘á»ƒ xem chi tiáº¿t...", None

        def clear_evidence():
            try:
                # Clear from memory
                fall_system.evidence_gifs.clear()
                fall_system.add_log("ğŸ—‘ï¸ ÄÃ£ xÃ³a danh sÃ¡ch báº±ng chá»©ng GIF", "info")
                display_text, evidence_choices, gif_paths = fall_system.get_evidence_gifs_display()
                return (
                    "âœ… ÄÃ£ xÃ³a danh sÃ¡ch báº±ng chá»©ng!",
                    display_text,
                    gr.update(choices=evidence_choices, value=None),
                    "Chá»n má»™t báº±ng chá»©ng tá»« dropdown Ä‘á»ƒ xem chi tiáº¿t...",
                    None,
                )
            except Exception as e:
                return f"âŒ Lá»—i xÃ³a báº±ng chá»©ng: {e}", "Lá»—i", gr.update(choices=[], value=None), "Lá»—i", None

        def on_evidence_select(selected_choice):
            if selected_choice is None:
                return "Chá»n má»™t báº±ng chá»©ng tá»« dropdown Ä‘á»ƒ xem chi tiáº¿t...", None

            # selected_choice is now just the text string
            details, gif_path = fall_system.get_evidence_details(selected_choice)
            return details, gif_path

        # Bind new events
        upload_btn.click(process_video, inputs=[video_upload], outputs=[upload_status, upload_info])

        refresh_evidence_btn.click(refresh_evidence, outputs=[evidence_summary, evidence_selector, evidence_details, evidence_gif_display])

        clear_evidence_btn.click(clear_evidence, outputs=[upload_status, evidence_summary, evidence_selector, evidence_details, evidence_gif_display])

        # Evidence selection event
        evidence_selector.change(on_evidence_select, inputs=[evidence_selector], outputs=[evidence_details, evidence_gif_display])

        # AI & Audio Control Event Handlers
        def on_detection_method_change(method):
            return fall_system.set_detection_method(method)

        def on_load_model():
            return fall_system.load_videollama3_model()

        def on_unload_model():
            return fall_system.unload_videollama3_model()

        def on_test_audio():
            return fall_system.test_audio_warning()

        def on_audio_enabled_change(enabled):
            return fall_system.set_audio_enabled(enabled)

        def on_audio_volume_change(volume):
            return fall_system.set_audio_volume(volume)

        # Bind AI & Audio events
        detection_method.change(on_detection_method_change, inputs=[detection_method], outputs=[method_output])

        load_model_btn.click(on_load_model, outputs=[model_output])

        unload_model_btn.click(on_unload_model, outputs=[model_output])

        test_audio_btn.click(on_test_audio, outputs=[audio_output])

        audio_enabled.change(on_audio_enabled_change, inputs=[audio_enabled], outputs=[audio_output])

        audio_volume.change(on_audio_volume_change, inputs=[audio_volume], outputs=[audio_output])

        # Faster refresh for camera feed (0.1s for real-time video)
        def update_camera():
            return fall_system.get_current_frame()

        # Enhanced status update with model status checking
        def update_status_and_logs_enhanced():
            # Check for UI updates from queue
            model_status_msg = ""
            try:
                while not fall_system.ui_update_queue.empty():
                    update_type, message = fall_system.ui_update_queue.get_nowait()
                    if update_type in ["model_loaded", "model_error", "model_unloaded"]:
                        model_status_msg = message
            except:
                pass

            if fall_system.is_running:
                evidence_text, evidence_choices, gif_paths = fall_system.get_evidence_gifs_display()
                if model_status_msg:
                    return (
                        fall_system.get_status_info(),
                        fall_system.get_logs_display(),
                        fall_system.get_alert_history_display(),
                        evidence_text,
                        gr.update(choices=evidence_choices),  # Only update choices, preserve value
                        gr.update(),  # evidence_details - don't change
                        gr.update(),  # evidence_gif_display - don't change
                        model_status_msg,
                    )
                else:
                    return (
                        fall_system.get_status_info(),
                        fall_system.get_logs_display(),
                        fall_system.get_alert_history_display(),
                        evidence_text,
                        gr.update(choices=evidence_choices),  # Only update choices, preserve value
                        gr.update(),  # evidence_details - don't change
                        gr.update(),  # evidence_gif_display - don't change
                        fall_system.get_model_status_message(),
                    )
            else:
                evidence_text, evidence_choices, gif_paths = fall_system.get_evidence_gifs_display()
                if model_status_msg:
                    return (
                        fall_system.get_status_info(),
                        gr.update(),
                        gr.update(),
                        evidence_text,
                        gr.update(choices=evidence_choices),
                        gr.update(),
                        gr.update(),
                        model_status_msg,
                    )
                else:
                    return (
                        fall_system.get_status_info(),
                        gr.update(),
                        gr.update(),
                        evidence_text,
                        gr.update(choices=evidence_choices),
                        gr.update(),
                        gr.update(),
                        fall_system.get_model_status_message(),
                    )

        # Set up dual auto-refresh timers using gr.Timer (Gradio 5.x)
        try:
            # Fast timer for camera feed (0.1s = 10 FPS)
            camera_timer = gr.Timer(0.1)
            camera_timer.tick(update_camera, outputs=[camera_feed])

            # Slower timer for status and logs (2s) with model status
            status_timer = gr.Timer(2.0)
            status_timer.tick(
                update_status_and_logs_enhanced,
                outputs=[status_display, logs_display, alert_display, evidence_summary, evidence_selector, evidence_details, evidence_gif_display, model_output],
            )

            print("âœ… Enhanced dual refresh timers set up: Camera 0.1s, Status/Logs 2s with model status")

        except Exception as e:
            print(f"âš ï¸ Enhanced auto-refresh timers not available: {e}")
            # Fallback: single timer for everything at 1s
            try:
                fallback_timer = gr.Timer(1.0)

                def update_all():
                    if fall_system.is_running:
                        evidence_text, evidence_choices, gif_paths = fall_system.get_evidence_gifs_display()
                        return (
                            fall_system.get_current_frame(),
                            fall_system.get_status_info(),
                            fall_system.get_logs_display(),
                            fall_system.get_alert_history_display(),
                            evidence_text,
                            gr.update(choices=evidence_choices),  # Only update choices, preserve value
                            gr.update(),  # evidence_details - don't change
                            gr.update(),  # evidence_gif_display - don't change
                            fall_system.get_model_status_message(),
                        )
                    else:
                        evidence_text, evidence_choices, gif_paths = fall_system.get_evidence_gifs_display()
                        return (
                            fall_system.get_current_frame(),
                            fall_system.get_status_info(),
                            gr.update(),
                            gr.update(),
                            evidence_text,
                            gr.update(choices=evidence_choices),  # Only update choices, preserve value
                            gr.update(),  # evidence_details - don't change
                            gr.update(),  # evidence_gif_display - don't change
                            fall_system.get_model_status_message(),
                        )

                fallback_timer.tick(
                    update_all,
                    outputs=[
                        camera_feed,
                        status_display,
                        logs_display,
                        alert_display,
                        evidence_summary,
                        evidence_selector,
                        evidence_details,
                        evidence_gif_display,
                        model_output,
                    ],
                )
                print("âš ï¸ Using enhanced fallback timer: All components 1s with model status")
            except:
                print("âŒ No auto-refresh available - manual refresh only")

    return demo


if __name__ == "__main__":
    # Create and launch the interface
    demo = create_interface()

    print("ğŸš€ Äang khá»Ÿi Ä‘á»™ng Gradio Web UI...")
    print("ğŸ“± Truy cáº­p giao diá»‡n web táº¡i: http://localhost:7860")
    print("ğŸ›‘ Nháº¥n Ctrl+C Ä‘á»ƒ dá»«ng server")

    try:
        demo.launch(server_name="0.0.0.0", server_port=7860, share=True, show_error=True, quiet=False, prevent_thread_lock=False)  # Use localhost first
    except Exception as e:
        print(f"âŒ Localhost launch failed: {e}")
        print("ğŸ”„ Trying with network access...")
        try:
            demo.launch(server_name="0.0.0.0", server_port=7860, share=False, show_error=True, quiet=False, prevent_thread_lock=False)  # Allow external access
        except Exception as e2:
            print(f"âŒ Network launch failed: {e2}")
            print("ğŸŒ Creating shareable link...")
            demo.launch(share=True, show_error=True, quiet=False, prevent_thread_lock=False)  # Create public link as last resort
